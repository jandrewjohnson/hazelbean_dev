name: Tests

on:
  push:
    branches: [main, develop, feature/*]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

# Prevent concurrent runs on the same branch
concurrency:
  group: tests-${{ github.ref }}
  cancel-in-progress: true

defaults:
  run:
    shell: bash -l {0}

env:
  CI: true
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  GDAL_DISABLE_READDIR_ON_OPEN: EMPTY_DIR
  GDAL_NUM_THREADS: 1
  GDAL_HTTP_TIMEOUT: 30

jobs:
  # Job 1: Core Tests (replaces Gates 1, 2, and 3)
  tests:
    name: Run Test Suite
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Hazelbean Environment
        uses: mamba-org/setup-micromamba@v2
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          cache-downloads: true
          
      - name: Verify environment
        run: |
          echo "🔍 Environment verification..."
          python --version
          python -c "import hazelbean as hb; print('✅ Hazelbean available')"
          
      - name: Run full test suite with coverage
        run: |
          echo "🧪 Running all tests..."
          
          # Run all tests with coverage
          python -m pytest hazelbean_tests/ \
            -v \
            --tb=short \
            --cov=hazelbean \
            --cov-report=term-missing \
            --cov-report=xml \
            --cov-report=html \
            --maxfail=10
            
      - name: Generate coverage summary
        if: always()
        run: |
          echo "## 📊 Test Coverage Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          coverage report >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          
      - name: Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: |
            htmlcov/
            coverage.xml
          retention-days: 30

  # Job 2: Performance Regression (kept from original)
  performance-regression:
    name: Performance Regression Check
    runs-on: ubuntu-22.04  # Lock OS version for consistency
    needs: tests
    if: ${{ !cancelled() }}
    timeout-minutes: 15
    
    outputs:
      performance-status: ${{ steps.performance.outputs.status }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Hazelbean Environment
        uses: mamba-org/setup-micromamba@v2
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          cache-downloads: true
      
      # Download baseline from main branch
      - name: Download baseline from main branch
        id: download_baseline
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: tests.yml
          branch: main
          name: performance-baseline
          path: metrics/baselines/
          if_no_artifact_found: warn
        continue-on-error: true
      
      # Validate downloaded baseline
      - name: Validate downloaded baseline
        id: validate_baseline
        run: |
          if [ -f metrics/baselines/current_performance_baseline.json ]; then
            echo "✅ Baseline downloaded from main branch"
            
            if python -c "import json; json.load(open('metrics/baselines/current_performance_baseline.json'))"; then
              echo "✅ Baseline JSON is valid"
              echo "baseline_source=main" >> $GITHUB_OUTPUT
              echo "baseline_valid=true" >> $GITHUB_OUTPUT
            else
              echo "❌ Downloaded baseline is corrupted"
              rm metrics/baselines/current_performance_baseline.json
              echo "baseline_valid=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "⚠️ No baseline available from main branch"
            echo "baseline_source=none" >> $GITHUB_OUTPUT
            echo "baseline_valid=false" >> $GITHUB_OUTPUT
          fi
      
      # Establish baseline with warmup if needed
      - name: Establish baseline with warmup
        if: steps.validate_baseline.outputs.baseline_valid != 'true'
        run: |
          echo "📊 Establishing baseline for this branch..."
          echo "⚠️ Using branch-local baseline (main baseline unavailable)"
          
          # Warmup run to stabilize system
          echo "🔥 Running warmup..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 1 > /dev/null 2>&1 || true
          
          # Actual baseline with 5 runs
          echo "📊 Establishing baseline with 5 runs..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 5
          
          echo "baseline_source=branch" >> $GITHUB_OUTPUT
          
      - name: Run performance regression check
        id: performance
        run: |
          echo "⚡ Running performance regression tests..."
          
          # Adaptive threshold based on baseline source
          if [[ "${{ steps.validate_baseline.outputs.baseline_source }}" == "main" ]]; then
            THRESHOLD=15.0
            echo "📊 Using 15% threshold (main branch baseline)"
          else
            THRESHOLD=20.0
            echo "⚠️ Using 20% threshold (branch-local baseline)"
            echo "::notice::Using branch-local baseline - results may be less reliable"
          fi
          
          # Run regression check
          python scripts/run_performance_benchmarks.py \
            --check-regression \
            --threshold $THRESHOLD \
            --verbose || {
              echo "❌ Performance regression > ${THRESHOLD}% detected"
              echo "📊 Baseline source: ${{ steps.validate_baseline.outputs.baseline_source }}"
              echo "status=failed" >> $GITHUB_OUTPUT
              exit 1
            }
          
          echo "✅ Performance within acceptable limits (${THRESHOLD}%)"
          echo "status=passed" >> $GITHUB_OUTPUT
          
      # Establish baseline for main branch
      - name: Establish baseline for main branch
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "📊 Establishing performance baseline for main branch..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 3
      
      # Upload baseline artifact
      - name: Upload baseline artifact
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: actions/upload-artifact@v4
        with:
          name: performance-baseline
          path: metrics/baselines/current_performance_baseline.json
          retention-days: 90
          overwrite: true
          
      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            metrics/reports/performance_report_*.txt
            metrics/exports/performance_export_*.csv
            metrics/analysis/regression_check_*.json
            metrics/baselines/current_performance_baseline.json
          retention-days: 90
      
      - name: Performance summary
        if: always() && github.event_name == 'pull_request'
        run: |
          echo "## ⚡ Performance Regression Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Baseline Source:** ${{ steps.validate_baseline.outputs.baseline_source }}" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ steps.performance.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f metrics/reports/performance_report_*.txt ]; then
            echo "### Performance Report Preview" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -20 metrics/reports/performance_report_*.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

  # Summary job
  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [tests, performance-regression]
    if: always()
    
    steps:
      - name: Evaluate results
        run: |
          echo "## 🎯 Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          
          # Tests
          if [[ "${{ needs.tests.result }}" == "success" ]]; then
            echo "| Test Suite | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Test Suite | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Performance
          perf_status="${{ needs.performance-regression.outputs.performance-status }}"
          if [[ "$perf_status" == "passed" ]]; then
            echo "| Performance | ✅ No regression |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Performance | ⚠️ Review needed |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Overall
          if [[ "${{ needs.tests.result }}" != "success" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "❌ **BLOCKING:** Test failures must be fixed" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ All critical checks passed" >> $GITHUB_STEP_SUMMARY

