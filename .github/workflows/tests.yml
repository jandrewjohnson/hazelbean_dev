name: Tests

on:
  push:
    branches: [main, develop, feature/*]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

# Prevent concurrent runs on the same branch
concurrency:
  group: tests-${{ github.ref }}
  cancel-in-progress: true

defaults:
  run:
    shell: bash -l {0}

env:
  CI: true
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  GDAL_DISABLE_READDIR_ON_OPEN: EMPTY_DIR
  GDAL_NUM_THREADS: 1
  GDAL_HTTP_TIMEOUT: 30

jobs:
  # Job 1: Core Tests (replaces Gates 1, 2, and 3)
  tests:
    name: Run Test Suite
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Hazelbean Environment
        uses: mamba-org/setup-micromamba@v2
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          cache-downloads: true
          
      - name: Verify environment
        run: |
          echo "ðŸ” Environment verification..."
          python --version
          python -c "import pytest; print('âœ… PyTest available')"
          
      - name: Verify PROJ database
        run: |
          echo "ðŸ—ºï¸ Verifying PROJ database setup..."
          python -c "
          import os
          import sys
          from osgeo import osr
          from pyproj import datadir
          
          # Check proj.db exists
          proj_data_dir = datadir.get_data_dir()
          proj_db = os.path.join(proj_data_dir, 'proj.db')
          
          print(f'PROJ data directory: {proj_data_dir}')
          print(f'proj.db path: {proj_db}')
          print(f'proj.db exists: {os.path.exists(proj_db)}')
          
          if not os.path.exists(proj_db):
              print('âŒ ERROR: proj.db not found!')
              print('This usually means proj-data package is not installed.')
              sys.exit(1)
          
          # Test EPSG import works
          try:
              srs = osr.SpatialReference()
              srs.ImportFromEPSG(4326)
              print('âœ… PROJ database verified - EPSG:4326 import successful')
          except Exception as e:
              print(f'âŒ ERROR: Cannot import EPSG:4326: {e}')
              sys.exit(1)
          "
          
      - name: Install hazelbean package
        run: |
          echo "ðŸ“¦ Installing hazelbean package (builds Cython extensions)..."
          python -m pip install -e . --no-deps
          
          # Verify installation and Cython extensions
          python -c "import hazelbean as hb; print('âœ… Hazelbean installed')"
          python -c "from hazelbean.calculation_core import cython_functions; print('âœ… Cython extensions ready')"
          
      - name: Run full test suite with coverage
        run: |
          echo "ðŸ§ª Running all tests..."
          
          # Run all tests with coverage
          python -m pytest hazelbean_tests/ \
            -v \
            --tb=short \
            --cov=hazelbean \
            --cov-report=term-missing \
            --cov-report=xml \
            --cov-report=html \
            --maxfail=10
            
      - name: Generate coverage summary
        if: always()
        run: |
          echo "## ðŸ“Š Test Coverage Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          coverage report >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          
      - name: Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: |
            htmlcov/
            coverage.xml
          retention-days: 30

  # Job 2: Performance Regression (kept from original)
  performance-regression:
    name: Performance Regression Check
    runs-on: ubuntu-22.04  # Lock OS version for consistency
    needs: tests
    if: ${{ !cancelled() }}
    timeout-minutes: 15
    
    outputs:
      performance-status: ${{ steps.performance.outputs.status }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Hazelbean Environment
        uses: mamba-org/setup-micromamba@v2
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          cache-downloads: true
      
      - name: Install hazelbean package
        run: |
          echo "ðŸ“¦ Installing hazelbean package for performance tests..."
          python -m pip install -e . --no-deps
      
      # Download baseline from main branch
      - name: Download baseline from main branch
        id: download_baseline
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: tests.yml
          branch: main
          name: performance-baseline
          path: metrics/baselines/
          if_no_artifact_found: warn
        continue-on-error: true
      
      # Validate downloaded baseline
      - name: Validate downloaded baseline
        id: validate_baseline
        run: |
          if [ -f metrics/baselines/current_performance_baseline.json ]; then
            echo "âœ… Baseline downloaded from main branch"
            
            if python -c "import json; json.load(open('metrics/baselines/current_performance_baseline.json'))"; then
              echo "âœ… Baseline JSON is valid"
              echo "baseline_source=main" >> $GITHUB_OUTPUT
              echo "baseline_valid=true" >> $GITHUB_OUTPUT
            else
              echo "âŒ Downloaded baseline is corrupted"
              rm metrics/baselines/current_performance_baseline.json
              echo "baseline_valid=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "âš ï¸ No baseline available from main branch"
            echo "baseline_source=none" >> $GITHUB_OUTPUT
            echo "baseline_valid=false" >> $GITHUB_OUTPUT
          fi
      
      # Establish baseline with warmup if needed
      - name: Establish baseline with warmup
        if: steps.validate_baseline.outputs.baseline_valid != 'true'
        run: |
          echo "ðŸ“Š Establishing baseline for this branch..."
          echo "âš ï¸ Using branch-local baseline (main baseline unavailable)"
          
          # Warmup run to stabilize system
          echo "ðŸ”¥ Running warmup..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 1 > /dev/null 2>&1 || true
          
          # Actual baseline with 5 runs
          echo "ðŸ“Š Establishing baseline with 5 runs..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 5
          
          echo "baseline_source=branch" >> $GITHUB_OUTPUT
          
      - name: Run performance regression check
        id: performance
        run: |
          echo "âš¡ Running performance regression tests..."
          
          # Use forgiving thresholds for CI environment variance
          if [[ "${{ steps.validate_baseline.outputs.baseline_source }}" == "main" ]]; then
            THRESHOLD=20.0
            echo "ðŸ“Š Using 20% threshold (main branch baseline)"
          else
            THRESHOLD=25.0
            echo "âš ï¸ Using 25% threshold (branch-local baseline)"
            echo "::notice::Using branch-local baseline - results may be less reliable"
          fi
          
          # Run regression check - warn but don't block CI
          python scripts/run_performance_benchmarks.py \
            --check-regression \
            --threshold $THRESHOLD \
            --verbose || {
              echo "âš ï¸ WARNING: Performance regression > ${THRESHOLD}% detected"
              echo "ðŸ“Š Baseline source: ${{ steps.validate_baseline.outputs.baseline_source }}"
              echo "::warning::Performance regression detected - not blocking CI"
              echo "status=warning" >> $GITHUB_OUTPUT
              exit 0
            }
          
          echo "âœ… Performance within acceptable limits (${THRESHOLD}%)"
          echo "status=passed" >> $GITHUB_OUTPUT
          
      # Establish baseline for main branch
      - name: Establish baseline for main branch
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "ðŸ“Š Establishing performance baseline for main branch..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 3
      
      # Upload baseline artifact
      - name: Upload baseline artifact
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: actions/upload-artifact@v4
        with:
          name: performance-baseline
          path: metrics/baselines/current_performance_baseline.json
          retention-days: 90
          overwrite: true
          
      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            metrics/reports/performance_report_*.txt
            metrics/exports/performance_export_*.csv
            metrics/analysis/regression_check_*.json
            metrics/baselines/current_performance_baseline.json
          retention-days: 90
      
      - name: Performance summary
        if: always() && github.event_name == 'pull_request'
        run: |
          echo "## âš¡ Performance Regression Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Baseline Source:** ${{ steps.validate_baseline.outputs.baseline_source }}" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ steps.performance.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f metrics/reports/performance_report_*.txt ]; then
            echo "### Performance Report Preview" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -20 metrics/reports/performance_report_*.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

  # Summary job
  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [tests, performance-regression]
    if: always()
    
    steps:
      - name: Evaluate results
        run: |
          echo "## ðŸŽ¯ Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Note:** Tests marked as xfail document known bugs in hazelbean core software." >> $GITHUB_STEP_SUMMARY
          echo "See \`hazelbean_tests/KNOWN_BUGS.md\` for details." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          
          # Tests
          if [[ "${{ needs.tests.result }}" == "success" ]]; then
            echo "| Test Suite | âœ… Passed (xfails documented) |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Test Suite | âŒ Failed |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Performance
          perf_status="${{ needs.performance-regression.outputs.performance-status }}"
          if [[ "$perf_status" == "passed" ]] || [[ "$perf_status" == "warning" ]]; then
            echo "| Performance | âœ… Within CI variance |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Performance | âš ï¸ Review needed |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Overall
          if [[ "${{ needs.tests.result }}" != "success" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âŒ **BLOCKING:** Test infrastructure failures must be fixed" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… All critical checks passed - CI infrastructure working correctly" >> $GITHUB_STEP_SUMMARY

