name: Tests

on:
  push:
    branches: [main, develop, feature/*]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

# Prevent concurrent runs on the same branch
concurrency:
  group: tests-${{ github.ref }}
  cancel-in-progress: true

defaults:
  run:
    shell: bash -l {0}

env:
  CI: true
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  GDAL_DISABLE_READDIR_ON_OPEN: EMPTY_DIR
  GDAL_NUM_THREADS: 1
  GDAL_HTTP_TIMEOUT: 30

jobs:
  # Job 1: Core Tests (replaces Gates 1, 2, and 3)
  tests:
    name: Run Test Suite
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Hazelbean Environment
        uses: mamba-org/setup-micromamba@v2
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          cache-downloads: true
          
      - name: Verify environment
        run: |
          echo "ðŸ” Environment verification..."
          python --version
          python -c "import pytest; print('âœ… PyTest available')"
          
      - name: Verify PROJ database
        run: |
          echo "ðŸ—ºï¸ Verifying PROJ database setup..."
          python -c "
          import os
          import sys
          from osgeo import osr
          from pyproj import datadir
          
          # Check proj.db exists
          proj_data_dir = datadir.get_data_dir()
          proj_db = os.path.join(proj_data_dir, 'proj.db')
          
          print(f'PROJ data directory: {proj_data_dir}')
          print(f'proj.db path: {proj_db}')
          print(f'proj.db exists: {os.path.exists(proj_db)}')
          
          if not os.path.exists(proj_db):
              print('âŒ ERROR: proj.db not found!')
              print('This usually means proj-data package is not installed.')
              sys.exit(1)
          
          # Test EPSG import works
          try:
              srs = osr.SpatialReference()
              srs.ImportFromEPSG(4326)
              print('âœ… PROJ database verified - EPSG:4326 import successful')
          except Exception as e:
              print(f'âŒ ERROR: Cannot import EPSG:4326: {e}')
              sys.exit(1)
          "
          
      - name: Set PROJ_LIB environment variable
        run: |
          echo "ðŸ”§ Setting PROJ_LIB for GDAL..."
          PROJ_LIB=$(python -c "from pyproj import datadir; print(datadir.get_data_dir())")
          echo "PROJ_LIB=$PROJ_LIB" >> $GITHUB_ENV
          echo "âœ… PROJ_LIB set to: $PROJ_LIB"
          
      - name: Set GDAL_DATA environment variable
        run: |
          echo "ðŸ—ºï¸ Setting GDAL_DATA for GDAL..."
          GDAL_DATA=$(python -c "import os; import sys; print(os.path.join(sys.prefix, 'share', 'gdal'))")
          # Verify GDAL_DATA directory exists and contains required files
          if [ ! -d "$GDAL_DATA" ]; then
            echo "âŒ ERROR: GDAL_DATA directory not found: $GDAL_DATA"
            exit 1
          fi
          if [ ! -f "$GDAL_DATA/gdalvrt.xsd" ] && [ ! -f "$GDAL_DATA/pcs.csv" ]; then
            echo "âš ï¸ WARNING: GDAL_DATA directory may not contain required files: $GDAL_DATA"
            echo "Listing contents:"
            ls -la "$GDAL_DATA" | head -10
          fi
          echo "GDAL_DATA=$GDAL_DATA" >> $GITHUB_ENV
          echo "âœ… GDAL_DATA set to: $GDAL_DATA"
          
      - name: Install hazelbean package
        run: |
          echo "ðŸ“¦ Installing hazelbean package (builds Cython extensions)..."
          python -m pip install -e . --no-deps
          
          # Verify installation and Cython extensions
          python -c "import hazelbean as hb; print('âœ… Hazelbean installed')"
          python -c "from hazelbean.calculation_core import cython_functions; print('âœ… Cython extensions ready')"
          
      - name: Run full test suite with coverage
        run: |
          echo "ðŸ§ª Running all tests..."
          
          # Run all tests with coverage
          python -m pytest hazelbean_tests/ \
            -v \
            --tb=short \
            --cov=hazelbean \
            --cov-report=term-missing \
            --cov-report=xml \
            --cov-report=html \
            --maxfail=10
            
      - name: Generate coverage summary
        if: always()
        run: |
          echo "## ðŸ“Š Test Coverage Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          coverage report >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          
      - name: Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: |
            htmlcov/
            coverage.xml
          retention-days: 30

  # Job 2: Performance Regression (kept from original)
  performance-regression:
    name: Performance Regression Check
    runs-on: ubuntu-22.04  # Lock OS version for consistency
    needs: tests
    if: ${{ !cancelled() }}
    timeout-minutes: 15
    
    outputs:
      performance-status: ${{ steps.performance.outputs.status }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Hazelbean Environment
        uses: mamba-org/setup-micromamba@v2
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          cache-downloads: true
      
      - name: Set PROJ_LIB environment variable
        run: |
          echo "ðŸ”§ Setting PROJ_LIB for GDAL..."
          PROJ_LIB=$(python -c "from pyproj import datadir; print(datadir.get_data_dir())")
          echo "PROJ_LIB=$PROJ_LIB" >> $GITHUB_ENV
          echo "âœ… PROJ_LIB set to: $PROJ_LIB"
      
      - name: Set GDAL_DATA environment variable
        run: |
          echo "ðŸ—ºï¸ Setting GDAL_DATA for GDAL..."
          GDAL_DATA=$(python -c "import os; import sys; print(os.path.join(sys.prefix, 'share', 'gdal'))")
          # Verify GDAL_DATA directory exists
          if [ ! -d "$GDAL_DATA" ]; then
            echo "âŒ ERROR: GDAL_DATA directory not found: $GDAL_DATA"
            exit 1
          fi
          echo "GDAL_DATA=$GDAL_DATA" >> $GITHUB_ENV
          echo "âœ… GDAL_DATA set to: $GDAL_DATA"
      
      - name: Install hazelbean package
        run: |
          echo "ðŸ“¦ Installing hazelbean package for performance tests..."
          python -m pip install -e . --no-deps
      
      # Download baseline from main branch
      - name: Download baseline from main branch
        id: download_baseline
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: tests.yml
          branch: main
          name: performance-baseline
          path: metrics/baselines/
          if_no_artifact_found: warn
        continue-on-error: true
      
      # Validate downloaded baseline
      - name: Validate downloaded baseline
        id: validate_baseline
        run: |
          if [ -f metrics/baselines/current_performance_baseline.json ]; then
            echo "âœ… Baseline downloaded from main branch"
            
            if python -c "import json; json.load(open('metrics/baselines/current_performance_baseline.json'))"; then
              echo "âœ… Baseline JSON is valid"
              echo "baseline_source=main" >> $GITHUB_OUTPUT
              echo "baseline_valid=true" >> $GITHUB_OUTPUT
            else
              echo "âŒ Downloaded baseline is corrupted"
              rm metrics/baselines/current_performance_baseline.json
              echo "baseline_valid=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "âš ï¸ No baseline available from main branch"
            echo "baseline_source=none" >> $GITHUB_OUTPUT
            echo "baseline_valid=false" >> $GITHUB_OUTPUT
          fi
      
      # Establish baseline with warmup if needed
      - name: Establish baseline with warmup
        if: steps.validate_baseline.outputs.baseline_valid != 'true'
        run: |
          echo "ðŸ“Š Establishing baseline for this branch..."
          echo "âš ï¸ Using branch-local baseline (main baseline unavailable)"
          
          # Warmup run to stabilize system
          echo "ðŸ”¥ Running warmup..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 1 > /dev/null 2>&1 || true
          
          # Actual baseline with 5 runs
          echo "ðŸ“Š Establishing baseline with 5 runs..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 5
          
          echo "baseline_source=branch" >> $GITHUB_OUTPUT
          
      - name: Run performance regression check
        id: performance
        run: |
          echo "âš¡ Running performance regression tests..."
          
          # Use forgiving thresholds for CI environment variance
          if [[ "${{ steps.validate_baseline.outputs.baseline_source }}" == "main" ]]; then
            THRESHOLD=20.0
            echo "ðŸ“Š Using 20% threshold (main branch baseline)"
          else
            THRESHOLD=25.0
            echo "âš ï¸ Using 25% threshold (branch-local baseline)"
            echo "::notice::Using branch-local baseline - results may be less reliable"
          fi
          
          # Run regression check - warn but don't block CI
          python scripts/run_performance_benchmarks.py \
            --check-regression \
            --threshold $THRESHOLD \
            --verbose || {
              echo "âš ï¸ WARNING: Performance regression > ${THRESHOLD}% detected"
              echo "ðŸ“Š Baseline source: ${{ steps.validate_baseline.outputs.baseline_source }}"
              echo "::warning::Performance regression detected - not blocking CI"
              echo "status=warning" >> $GITHUB_OUTPUT
              exit 0
            }
          
          echo "âœ… Performance within acceptable limits (${THRESHOLD}%)"
          echo "status=passed" >> $GITHUB_OUTPUT
          
      # Establish baseline for main branch
      - name: Establish baseline for main branch
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        continue-on-error: true  # Don't block merges if baseline fails
        run: |
          echo "ðŸ“Š Establishing performance baseline for main branch..."
          
          # Run diagnostic first to see what's happening
          echo "ðŸ” Running pytest diagnostic..."
          python scripts/diagnose_pytest_benchmark.py || true
          
          echo ""
          echo "ðŸš€ Running actual baseline establishment..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 3 --verbose || {
            echo "âš ï¸  WARNING: Baseline establishment failed"
            echo "This is not blocking - the tests use custom timing instead of pytest-benchmark fixtures"
            exit 0
          }
      
      # Upload baseline artifact
      - name: Upload baseline artifact
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: actions/upload-artifact@v4
        with:
          name: performance-baseline
          path: metrics/baselines/current_performance_baseline.json
          retention-days: 90
          overwrite: true
          
      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            metrics/reports/performance_report_*.txt
            metrics/exports/performance_export_*.csv
            metrics/analysis/regression_check_*.json
            metrics/baselines/current_performance_baseline.json
          retention-days: 90
      
      - name: Performance summary
        if: always() && github.event_name == 'pull_request'
        run: |
          echo "## âš¡ Performance Regression Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Baseline Source:** ${{ steps.validate_baseline.outputs.baseline_source }}" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ steps.performance.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f metrics/reports/performance_report_*.txt ]; then
            echo "### Performance Report Preview" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -20 metrics/reports/performance_report_*.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

  # Summary job
  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [tests, performance-regression]
    if: always()
    
    steps:
      - name: Evaluate results
        run: |
          echo "## ðŸŽ¯ Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Note:** Tests marked as xfail document known bugs in hazelbean core software." >> $GITHUB_STEP_SUMMARY
          echo "See \`hazelbean_tests/KNOWN_BUGS.md\` for details." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          
          # Tests - handle missing/cancelled jobs gracefully
          TESTS_RESULT="${{ needs.tests.result }}"
          if [[ -z "$TESTS_RESULT" ]] || [[ "$TESTS_RESULT" == "null" ]]; then
            TESTS_RESULT="unknown"
          fi
          
          if [[ "$TESTS_RESULT" == "success" ]]; then
            echo "| Test Suite | âœ… Passed (xfails documented) |" >> $GITHUB_STEP_SUMMARY
          elif [[ "$TESTS_RESULT" == "failure" ]] || [[ "$TESTS_RESULT" == "cancelled" ]]; then
            echo "| Test Suite | âŒ Failed |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Test Suite | âš ï¸ Status: $TESTS_RESULT |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Performance - handle missing outputs gracefully
          PERF_STATUS="${{ needs.performance-regression.outputs.performance-status }}"
          if [[ -z "$PERF_STATUS" ]] || [[ "$PERF_STATUS" == "null" ]]; then
            PERF_STATUS="unknown"
          fi
          
          if [[ "$PERF_STATUS" == "passed" ]] || [[ "$PERF_STATUS" == "warning" ]]; then
            echo "| Performance | âœ… Within CI variance |" >> $GITHUB_STEP_SUMMARY
          elif [[ "$PERF_STATUS" == "unknown" ]]; then
            echo "| Performance | âš ï¸ Status unavailable |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Performance | âš ï¸ Review needed |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Overall - only exit with error if tests actually failed
          if [[ "$TESTS_RESULT" == "failure" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âŒ **BLOCKING:** Test infrastructure failures must be fixed" >> $GITHUB_STEP_SUMMARY
            exit 1
          elif [[ "$TESTS_RESULT" == "success" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âœ… All critical checks passed - CI infrastructure working correctly" >> $GITHUB_STEP_SUMMARY
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âš ï¸ **NOTE:** Test status was $TESTS_RESULT - may need investigation" >> $GITHUB_STEP_SUMMARY
          fi

