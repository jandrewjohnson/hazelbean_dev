# Testing Quality Gates Workflow
# 
# Comprehensive testing pipeline implementing quality gates for hazelbean development
# Demonstrates CI/CD integration patterns for hazelbean testing infrastructure
#
# Usage: Copy this workflow to your .github/workflows/ directory to implement
#        systematic quality gate testing for hazelbean projects

name: Testing Quality Gates

on:
  push:
    branches: [ main, develop ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'
  pull_request:
    branches: [ main, develop ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'
  workflow_dispatch: # Allow manual triggering
    inputs:
      test_level:
        description: 'Test level to run'
        required: false
        default: 'full'
        type: choice
        options:
          - 'smoke'
          - 'core'
          - 'full'

# Prevent concurrent runs on the same branch to avoid resource conflicts
concurrency:
  group: testing-${{ github.ref }}
  cancel-in-progress: true

defaults:
  run:
    shell: bash -l {0}

env:
  # CI environment configuration
  CI: true
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  # GDAL configuration for stability
  GDAL_DISABLE_READDIR_ON_OPEN: EMPTY_DIR
  GDAL_NUM_THREADS: 1
  GDAL_HTTP_TIMEOUT: 30

jobs:
  
  # Quality Gate 1: Infrastructure Validation
  infrastructure-validation:
    name: "Quality Gate 1: Infrastructure"
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    outputs:
      infrastructure-status: ${{ steps.infrastructure.outputs.status }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          
      - name: Setup Mambaforge with hazelbean_env
        uses: mamba-org/setup-micromamba@v2
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          cache-downloads: true
          init-shell: bash
          
      - name: Verify environment setup
        run: |
          echo "🔍 Verifying environment setup..."
          conda info --envs
          which python
          python --version
          python -c "import hazelbean as hb; print('✅ Hazelbean available')" || echo "❌ Hazelbean import failed"
          python -c "import pytest; print('✅ PyTest available')" || echo "❌ PyTest import failed"
          
      - name: Infrastructure smoke test
        id: infrastructure
        run: |
          echo "🚀 Running infrastructure validation..."
          
          # Run smoke tests with timeout protection
          timeout 300 python -m pytest hazelbean_tests/system/test_smoke.py -v --tb=short || {
            echo "status=failed" >> $GITHUB_OUTPUT
            echo "❌ Infrastructure smoke tests failed"
            exit 1
          }
          
          echo "status=passed" >> $GITHUB_OUTPUT
          echo "✅ Infrastructure validation passed"

  # Quality Gate 2: Core Functionality Testing
  core-functionality:
    name: "Quality Gate 2: Core Functionality"
    runs-on: ubuntu-latest
    needs: infrastructure-validation
    if: needs.infrastructure-validation.outputs.infrastructure-status == 'passed'
    timeout-minutes: 20
    
    strategy:
      fail-fast: false
      matrix:
        test-category: ['get_path', 'tile_iterator', 'add_task', 'add_iterator']
        # Note: add_task includes 2 xfail tests documenting known bug in project_flow.py
        # These tests will show as xfailed (not failed) in CI output
        # See hazelbean_tests/KNOWN_BUGS.md #add_task_error_handling for details
        
    outputs:
      core-status: ${{ steps.core-tests.outputs.status }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Mambaforge with hazelbean_env  
        uses: mamba-org/setup-micromamba@v2
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          cache-downloads: true
          
      - name: Run core functionality tests
        id: core-tests
        run: |
          echo "🎯 Testing core functionality: ${{ matrix.test-category }}"
          
          # Run with coverage and JSON report for failure tracking
          python -m pytest hazelbean_tests/unit/test_${{ matrix.test-category }}.py -v \
            --tb=short \
            --cov=hazelbean \
            --cov-report=term-missing \
            --cov-report=xml:coverage-${{ matrix.test-category }}.xml \
            --json-report \
            --json-report-file=test-results-${{ matrix.test-category }}.json \
            --maxfail=3 || {
              echo "status=failed" >> $GITHUB_OUTPUT
              echo "❌ Core functionality tests failed for ${{ matrix.test-category }}"
              exit 1
            }
          
          echo "status=passed" >> $GITHUB_OUTPUT
          echo "✅ Core functionality tests passed for ${{ matrix.test-category }}"

      - name: Generate failure rate report
        if: always()
        run: |
          echo "📊 Generating test failure rate report..."
          python tools/test_failure_tracking.py test-results-${{ matrix.test-category }}.json \
            --compare-to metrics/test-failures/baseline-${{ matrix.test-category }}.json \
            --save-baseline metrics/test-failures/current-${{ matrix.test-category }}.json || true
          
          # Add summary to GitHub Actions output
          python -c "
          import json
          try:
              with open('test-results-${{ matrix.test-category }}.json') as f:
                  data = json.load(f)
                  summary = data.get('summary', {})
                  print(f'::notice::Tests: {summary.get(\"total\", 0)} total, {summary.get(\"passed\", 0)} passed, {summary.get(\"xfailed\", 0)} known bugs')
          except Exception as e:
              print(f'::warning::Could not parse test results: {e}')
          "
          
      - name: Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.test-category }}
          path: coverage-${{ matrix.test-category }}.xml
          retention-days: 30
          
      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test-category }}
          path: |
            test-results-${{ matrix.test-category }}.json
            metrics/test-failures/current-${{ matrix.test-category }}.json
          retention-days: 90

  # Quality Gate 3: Integration Testing
  integration-testing:
    name: "Quality Gate 3: Integration Testing"
    runs-on: ubuntu-latest
    needs: [infrastructure-validation, core-functionality]
    if: needs.infrastructure-validation.outputs.infrastructure-status == 'passed'
    timeout-minutes: 30
    
    outputs:
      integration-status: ${{ steps.integration.outputs.status }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Mambaforge with hazelbean_env
        uses: mamba-org/setup-micromamba@v2
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          cache-downloads: true
          
      - name: Run integration tests
        id: integration  
        run: |
          echo "🔄 Running integration workflow tests..."
          
          # Run most reliable integration tests first (including comprehensive ProjectFlow tests)
          python -m pytest \
            hazelbean_tests/integration/test_project_flow_task_management.py \
            hazelbean_tests/integration/test_data_processing.py::TestGetPathIntegration \
            hazelbean_tests/integration/test_end_to_end_workflow.py \
            -v --tb=short --maxfail=5 || {
              echo "❌ Critical integration tests failed"
              echo "status=warning" >> $GITHUB_OUTPUT
              # Don't fail the job - integration issues are often infrastructure-related
            }
            
          # Run broader integration suite with failure tolerance
          python -m pytest hazelbean_tests/integration/ -v \
            --tb=short \
            --maxfail=20 || {
              echo "⚠️ Some integration tests failed (acceptable for infrastructure issues)"
            }
          
          echo "status=completed" >> $GITHUB_OUTPUT
          echo "✅ Integration testing completed"

  # Quality Gate 4: Performance Regression Testing
  performance-regression:
    name: "Quality Gate 4: Performance Regression"
    runs-on: ubuntu-22.04  # Lock specific OS version for consistency
    needs: core-functionality
    if: ${{ !cancelled() && (needs.core-functionality.result == 'success' || github.event.inputs.test_level == 'full') }}
    timeout-minutes: 15
    
    outputs:
      performance-status: ${{ steps.performance.outputs.status }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Mambaforge with hazelbean_env
        uses: mamba-org/setup-micromamba@v2
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          cache-downloads: true
      
      # Download baseline from main branch for regression comparison
      - name: Download baseline from main branch
        id: download_baseline
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: testing-quality-gates.yml
          branch: main
          name: performance-baseline
          path: metrics/baselines/
          if_no_artifact_found: warn
        continue-on-error: true
      
      # Validate downloaded baseline
      - name: Validate downloaded baseline
        id: validate_baseline
        run: |
          if [ -f metrics/baselines/current_performance_baseline.json ]; then
            echo "✅ Baseline downloaded from main branch"
            
            # Validate JSON structure
            if python -c "import json; json.load(open('metrics/baselines/current_performance_baseline.json'))"; then
              echo "✅ Baseline JSON is valid"
              echo "baseline_source=main" >> $GITHUB_OUTPUT
              echo "baseline_valid=true" >> $GITHUB_OUTPUT
            else
              echo "❌ Downloaded baseline is corrupted"
              rm metrics/baselines/current_performance_baseline.json
              echo "baseline_valid=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "⚠️ No baseline available from main branch"
            echo "baseline_source=none" >> $GITHUB_OUTPUT
            echo "baseline_valid=false" >> $GITHUB_OUTPUT
          fi
      
      # Establish baseline with warmup if needed
      - name: Establish baseline with warmup
        if: steps.validate_baseline.outputs.baseline_valid != 'true'
        run: |
          echo "📊 Establishing baseline for this branch..."
          echo "⚠️ WARNING: Using branch-local baseline (main baseline unavailable)"
          
          # Warmup run to stabilize cache/system state
          echo "🔥 Running warmup to stabilize system..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 1 > /dev/null 2>&1 || true
          
          # Actual baseline establishment with more runs for reliability
          echo "📊 Establishing baseline with 5 runs..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 5
          
          echo "baseline_source=branch" >> $GITHUB_OUTPUT
          
      - name: Run performance regression tests with adaptive threshold
        id: performance
        run: |
          echo "⚡ Running performance regression tests..."
          
          # Use different thresholds based on baseline source
          if [[ "${{ steps.validate_baseline.outputs.baseline_source }}" == "main" ]]; then
            THRESHOLD=15.0
            echo "📊 Using 15% threshold (comparing against main branch baseline)"
          else
            THRESHOLD=20.0
            echo "⚠️ Using 20% threshold (comparing against branch-local baseline)"
            echo "::notice::Performance comparison uses branch-local baseline - results may be less reliable"
          fi
          
          # Run regression check
          python scripts/run_performance_benchmarks.py \
            --check-regression \
            --threshold $THRESHOLD \
            --verbose || {
              echo "❌ BLOCKING: Performance regression > ${THRESHOLD}% detected"
              echo "📊 Baseline source: ${{ steps.validate_baseline.outputs.baseline_source }}"
              echo "🛑 Review performance report for details"
              echo "status=failed" >> $GITHUB_OUTPUT
              exit 1
            }
          
          echo "✅ Performance within acceptable limits (threshold: ${THRESHOLD}%)"
          echo "status=passed" >> $GITHUB_OUTPUT
          
      - name: Establish baseline for main branch
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "📊 Establishing performance baseline for main branch..."
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 3
      
      # Upload baseline artifact for PR branches to use
      - name: Upload baseline artifact for future runs
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: actions/upload-artifact@v4
        with:
          name: performance-baseline
          path: metrics/baselines/current_performance_baseline.json
          retention-days: 90
          overwrite: true
          
      - name: Upload performance artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-regression-results
          path: |
            metrics/reports/performance_report_*.txt
            metrics/exports/performance_export_*.csv
            metrics/analysis/regression_check_*.json
            metrics/baselines/current_performance_baseline.json
          retention-days: 90
      
      - name: Add performance summary to PR
        if: always() && github.event_name == 'pull_request'
        run: |
          echo "## ⚡ Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Baseline Source:** ${{ steps.validate_baseline.outputs.baseline_source }}" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ steps.performance.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f metrics/reports/performance_report_*.txt ]; then
            echo "### Performance Report Preview" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -20 metrics/reports/performance_report_*.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

  # Quality Gate Summary & Reporting
  quality-gate-summary:
    name: "Quality Gate Summary"
    runs-on: ubuntu-latest
    needs: [infrastructure-validation, core-functionality, integration-testing, performance-regression]
    if: always() # Always run to provide summary
    
    steps:
      - name: Evaluate quality gates
        id: evaluation
        run: |
          echo "📊 Evaluating quality gate results..."
          
          # Extract results
          infra_status="${{ needs.infrastructure-validation.outputs.infrastructure-status }}"
          core_status="${{ needs.core-functionality.outputs.core-status }}"  
          integration_status="${{ needs.integration-testing.outputs.integration-status }}"
          performance_status="${{ needs.performance-regression.outputs.performance-status }}"
          
          # Determine overall status
          if [[ "$infra_status" == "passed" && "$core_status" == "passed" ]]; then
            echo "overall_status=success" >> $GITHUB_OUTPUT
            echo "quality_level=high" >> $GITHUB_OUTPUT
          elif [[ "$infra_status" == "passed" ]]; then
            echo "overall_status=warning" >> $GITHUB_OUTPUT  
            echo "quality_level=medium" >> $GITHUB_OUTPUT
          else
            echo "overall_status=failure" >> $GITHUB_OUTPUT
            echo "quality_level=low" >> $GITHUB_OUTPUT
          fi
          
          # Generate summary
          echo "## 🎯 Quality Gate Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Note:** Some tests marked as xfail document known bugs in original hazelbean code." >> $GITHUB_STEP_SUMMARY
          echo "These are expected and tracked in KNOWN_BUGS.md" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Quality Gate | Status | Result |" >> $GITHUB_STEP_SUMMARY
          echo "|--------------|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Infrastructure | $infra_status | $([ "$infra_status" == "passed" ] && echo "✅ All systems operational" || echo "❌ Infrastructure issues") |" >> $GITHUB_STEP_SUMMARY
          echo "| Core Functionality | $core_status | $([ "$core_status" == "passed" ] && echo "✅ Core features working" || echo "❌ Core feature issues") |" >> $GITHUB_STEP_SUMMARY  
          echo "| Integration Testing | $integration_status | $([ "$integration_status" == "completed" ] && echo "⚠️ Completed with known issues" || echo "❌ Integration problems") |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Regression | $performance_status | $([ "$performance_status" == "completed" ] && echo "✅ No significant regressions" || echo "⚠️ Performance needs review") |" >> $GITHUB_STEP_SUMMARY
          
      - name: Set job status based on critical gates
        run: |
          # Only fail on critical issues (infrastructure or core functionality)
          infra_status="${{ needs.infrastructure-validation.outputs.infrastructure-status }}"
          core_status="${{ needs.core-functionality.outputs.core-status }}"
          
          if [[ "$infra_status" != "passed" ]]; then
            echo "❌ BLOCKING: Infrastructure validation failed"
            exit 1
          fi
          
          if [[ "$core_status" != "passed" ]]; then
            echo "❌ BLOCKING: Core functionality validation failed"  
            exit 1
          fi
          
          echo "✅ All critical quality gates passed"

  # Conditional deployment readiness
  deployment-readiness:
    name: "Deployment Readiness Check"
    runs-on: ubuntu-latest
    needs: quality-gate-summary
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
      - name: Deployment readiness assessment
        run: |
          echo "🚀 Assessing deployment readiness..."
          echo "## 🚀 Deployment Readiness" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ All critical quality gates passed" >> $GITHUB_STEP_SUMMARY
          echo "📦 Ready for deployment pipeline" >> $GITHUB_STEP_SUMMARY
          echo "🔍 Review performance and integration warnings before release" >> $GITHUB_STEP_SUMMARY
