{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"System Tests\"\n",
        "execute:\n",
        "  enabled: false\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    code-line-numbers: true\n",
        "    code-copy: true\n",
        "    highlight-style: github\n",
        "    toc: true\n",
        "---\n",
        "\n",
        "System tests validate the complete hazelbean system, including smoke tests to verify basic functionality and end-to-end workflows.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The system test suite covers:\n",
        "\n",
        "- **Smoke Tests** - Quick validation that core features work\n",
        "- **Workflow Tests** - Complete ProjectFlow workflows\n",
        "- **Environment Tests** - Testing in different environments\n",
        "- **Installation Tests** - Verifying correct installation\n",
        "\n",
        "---\n",
        "\n",
        "## Project Flow Workflows Tests\n",
        "\n",
        "**Source File:** `hazelbean_tests/system/test_project_flow_workflows.py`\n",
        "\n",
        "### test_complete_project_creation_to_cleanup_lifecycle()\n",
        "\n",
        "Test full project lifecycle from creation to cleanup.\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    def test_complete_project_creation_to_cleanup_lifecycle(self, temp_dir):\n",
        "        \"\"\"Test full project lifecycle from creation to cleanup.\"\"\"\n",
        "        lifecycle_log = []\n",
        "        \n",
        "        # Phase 1: Project Creation\n",
        "        project_dir = os.path.join(temp_dir, 'test_project')\n",
        "        p = hb.ProjectFlow(project_dir)\n",
        "        \n",
        "        lifecycle_log.append(\"project_created\")\n",
        "        \n",
        "        # Validate project directories were created\n",
        "        assert os.path.exists(p.project_dir)\n",
        "        # Note: intermediate_dir is created during execution, not at project creation\n",
        "        \n",
        "        # Phase 2: Task Definition\n",
        "        def data_preparation_task(p):\n",
        "            \"\"\"Prepare initial data files.\"\"\"\n",
        "            lifecycle_log.append(\"data_preparation_executed\")\n",
        "            \n",
        "            # Create input data\n",
        "            input_file = os.path.join(p.cur_dir, 'input_data.txt')\n",
        "            with open(input_file, 'w') as f:\n",
        "                f.write(\"Initial data for processing\")\n",
        "        \n",
        "        def processing_iterator(p):\n",
        "            \"\"\"Process data in multiple scenarios.\"\"\"\n",
        "            lifecycle_log.append(\"processing_iterator_executed\")\n",
        "            \n",
        "            # Set up processing scenarios\n",
        "            p.iterator_replacements = {\n",
        "                'process_type': ['clean', 'validate', 'transform'],\n",
        "                'cur_dir_parent_dir': [\n",
        "                    os.path.join(p.intermediate_dir, 'clean'),\n",
        "                    os.path.join(p.intermediate_dir, 'validate'),\n",
        "                    os.path.join(p.intermediate_dir, 'transform')\n",
        "                ]\n",
        "            }\n",
        "            \n",
        "            for dir_path in p.iterator_replacements['cur_dir_parent_dir']:\n",
        "                os.makedirs(dir_path, exist_ok=True)\n",
        "        \n",
        "        def scenario_processing_task(p):\n",
        "            \"\"\"Process each scenario.\"\"\"\n",
        "            lifecycle_log.append(f\"scenario_processing_executed_{p.process_type}\")\n",
        "            \n",
        "            # Read input data\n",
        "            input_file = os.path.join(p.data_preparation_task_dir, 'input_data.txt')\n",
        "            if os.path.exists(input_file):\n",
        "                with open(input_file, 'r') as f:\n",
        "                    input_data = f.read()\n",
        "                \n",
        "                # Create scenario output\n",
        "                output_file = os.path.join(p.cur_dir, f'{p.process_type}_result.txt')\n",
        "                with open(output_file, 'w') as f:\n",
        "                    f.write(f\"{p.process_type.title()} processing: {input_data}\")\n",
        "            else:\n",
        "                pytest.fail(f\"Input data not found for {p.process_type} processing\")\n",
        "        \n",
        "        def results_compilation_task(p):\n",
        "            \"\"\"Compile all processing results.\"\"\"\n",
        "            lifecycle_log.append(\"results_compilation_executed\")\n",
        "            \n",
        "            # Collect results from all scenarios - files are in task subdirectories\n",
        "            scenario_results = []\n",
        "            for process_type in ['clean', 'validate', 'transform']:\n",
        "                result_file = os.path.join(\n",
        "                    p.intermediate_dir, process_type, 'scenario_processing_task', f'{process_type}_result.txt'\n",
        "                )\n",
        "                if os.path.exists(result_file):\n",
        "                    with open(result_file, 'r') as f:\n",
        "                        scenario_results.append(f.read())\n",
        "            \n",
        "            # Create final compilation\n",
        "            final_output = os.path.join(p.cur_dir, 'final_results.txt')\n",
        "            with open(final_output, 'w') as f:\n",
        "                f.write(\"Compiled Results:\\n\")\n",
        "                f.write(\"\\n\".join(scenario_results))\n",
        "        \n",
        "        # Build project workflow\n",
        "        data_prep = p.add_task(data_preparation_task)\n",
        "        processing = p.add_iterator(processing_iterator, run_in_parallel=False)\n",
        "        scenario_task = p.add_task(scenario_processing_task, parent=processing)\n",
        "        results = p.add_task(results_compilation_task)\n",
        "        \n",
        "        lifecycle_log.append(\"tasks_defined\")\n",
        "        \n",
        "        # Phase 3: Execution\n",
        "        p.execute()\n",
        "        lifecycle_log.append(\"execution_completed\")\n",
        "        \n",
        "        # Phase 4: Validation\n",
        "        assert len(lifecycle_log) == 9  # 1 created + 1 defined + 1 data_prep + 1 iterator + 3 scenarios + 1 compilation + 1 execution_completed\n",
        "        \n",
        "        # Validate execution order\n",
        "        expected_order = [\n",
        "            \"project_created\",\n",
        "            \"tasks_defined\", \n",
        "            \"data_preparation_executed\",\n",
        "            \"processing_iterator_executed\",\n",
        "            \"scenario_processing_executed_clean\",\n",
        "            \"scenario_processing_executed_validate\", \n",
        "            \"scenario_processing_executed_transform\",\n",
        "            \"results_compilation_executed\",\n",
        "            \"execution_completed\"\n",
        "        ]\n",
        "        \n",
        "        for i, expected in enumerate(expected_order):\n",
        "            assert lifecycle_log[i].startswith(expected) or lifecycle_log[i] == expected\n",
        "        \n",
        "        # Validate file outputs\n",
        "        input_file = os.path.join(p.data_preparation_task_dir, 'input_data.txt')\n",
        "        final_results = os.path.join(p.results_compilation_task_dir, 'final_results.txt')\n",
        "        \n",
        "        assert os.path.exists(input_file)\n",
        "        assert os.path.exists(final_results)\n",
        "        \n",
        "        # Validate final results content\n",
        "        with open(final_results, 'r') as f:\n",
        "            content = f.read()\n",
        "            assert \"Clean processing: Initial data\" in content\n",
        "            assert \"Validate processing: Initial data\" in content\n",
        "            assert \"Transform processing: Initial data\" in content\n",
        "        \n",
        "        # Phase 5: Cleanup (handled by temp_dir fixture)\n",
        "        lifecycle_log.append(\"cleanup_ready\")\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_multi_stage_project_workflow_with_dependencies()\n",
        "\n",
        "Test complex multi-stage project with task dependencies.\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    def test_multi_stage_project_workflow_with_dependencies(self, temp_dir):\n",
        "        \"\"\"Test complex multi-stage project with task dependencies.\"\"\"\n",
        "        stage_tracking = {'current_stage': 0, 'completed_stages': []}\n",
        "        \n",
        "        project_dir = os.path.join(temp_dir, 'multi_stage_project')\n",
        "        p = hb.ProjectFlow(project_dir)\n",
        "        \n",
        "        def stage_1_setup(p):\n",
        "            \"\"\"Stage 1: Initial setup and configuration.\"\"\"\n",
        "            stage_tracking['current_stage'] = 1\n",
        "            stage_tracking['completed_stages'].append(1)\n",
        "            \n",
        "            config_file = os.path.join(p.cur_dir, 'project_config.txt')\n",
        "            with open(config_file, 'w') as f:\n",
        "                f.write(\"project_version=1.0\\nprocessing_mode=batch\\noutput_format=csv\")\n",
        "        \n",
        "        def stage_2_data_collection_iterator(p):\n",
        "            \"\"\"Stage 2: Collect data from multiple sources.\"\"\"\n",
        "            stage_tracking['current_stage'] = 2\n",
        "            stage_tracking['completed_stages'].append(2)\n",
        "            \n",
        "            # Verify stage 1 completed\n",
        "            config_file = os.path.join(p.stage_1_setup_dir, 'project_config.txt')\n",
        "            assert os.path.exists(config_file), \"Stage 1 dependency not met\"\n",
        "            \n",
        "            # Set up data collection scenarios\n",
        "            p.iterator_replacements = {\n",
        "                'data_source': ['database', 'api', 'files'],\n",
        "                'cur_dir_parent_dir': [\n",
        "                    os.path.join(p.cur_dir, 'database_collection'),\n",
        "                    os.path.join(p.cur_dir, 'api_collection'),\n",
        "                    os.path.join(p.cur_dir, 'files_collection')\n",
        "                ]\n",
        "            }\n",
        "            \n",
        "            for dir_path in p.iterator_replacements['cur_dir_parent_dir']:\n",
        "                os.makedirs(dir_path, exist_ok=True)\n",
        "        \n",
        "        def data_collection_task(p):\n",
        "            \"\"\"Collect data from specific source.\"\"\"\n",
        "            collected_file = os.path.join(p.cur_dir, f'{p.data_source}_data.txt')\n",
        "            with open(collected_file, 'w') as f:\n",
        "                f.write(f\"Data collected from {p.data_source} source\")\n",
        "        \n",
        "        def stage_3_processing(p):\n",
        "            \"\"\"Stage 3: Process all collected data.\"\"\"\n",
        "            stage_tracking['current_stage'] = 3\n",
        "            stage_tracking['completed_stages'].append(3)\n",
        "            \n",
        "            # Verify stage 2 dependencies - files are in task subdirectories\n",
        "            required_files = [\n",
        "                'database_collection/data_collection_task/database_data.txt',\n",
        "                'api_collection/data_collection_task/api_data.txt', \n",
        "                'files_collection/data_collection_task/files_data.txt'\n",
        "            ]\n",
        "            \n",
        "            stage_2_dir = p.stage_2_data_collection_iterator_dir\n",
        "            for req_file in required_files:\n",
        "                full_path = os.path.join(stage_2_dir, req_file)\n",
        "                assert os.path.exists(full_path), f\"Stage 2 dependency missing: {req_file}\"\n",
        "            \n",
        "            # Process all data\n",
        "            processed_data = []\n",
        "            for req_file in required_files:\n",
        "                full_path = os.path.join(stage_2_dir, req_file)\n",
        "                with open(full_path, 'r') as f:\n",
        "                    processed_data.append(f.read())\n",
        "            \n",
        "            # Create processed output\n",
        "            output_file = os.path.join(p.cur_dir, 'processed_data.txt')\n",
        "            with open(output_file, 'w') as f:\n",
        "                f.write(\"Processed Data:\\n\")\n",
        "                f.write(\"\\n\".join(processed_data))\n",
        "        \n",
        "        def stage_4_final_output(p):\n",
        "            \"\"\"Stage 4: Generate final project outputs.\"\"\"\n",
        "            stage_tracking['current_stage'] = 4\n",
        "            stage_tracking['completed_stages'].append(4)\n",
        "            \n",
        "            # Verify all previous stage dependencies\n",
        "            dependencies = [\n",
        "                (p.stage_1_setup_dir, 'project_config.txt'),\n",
        "                (p.stage_3_processing_dir, 'processed_data.txt')\n",
        "            ]\n",
        "            \n",
        "            for dep_dir, dep_file in dependencies:\n",
        "                dep_path = os.path.join(dep_dir, dep_file)\n",
        "                assert os.path.exists(dep_path), f\"Dependency missing: {dep_path}\"\n",
        "            \n",
        "            # Generate final output\n",
        "            final_output = os.path.join(p.cur_dir, 'project_final_output.txt')\n",
        "            with open(final_output, 'w') as f:\n",
        "                f.write(\"Project completed successfully\\n\")\n",
        "                f.write(f\"Completed stages: {stage_tracking['completed_stages']}\\n\")\n",
        "                f.write(f\"Final stage: {stage_tracking['current_stage']}\")\n",
        "        \n",
        "        # Build multi-stage workflow\n",
        "        stage_1 = p.add_task(stage_1_setup)\n",
        "        stage_2_iterator = p.add_iterator(stage_2_data_collection_iterator, run_in_parallel=False)\n",
        "        collection_task = p.add_task(data_collection_task, parent=stage_2_iterator)\n",
        "        stage_3 = p.add_task(stage_3_processing)\n",
        "        stage_4 = p.add_task(stage_4_final_output)\n",
        "        \n",
        "        # Execute complete workflow\n",
        "        p.execute()\n",
        "        \n",
        "        # Validate workflow completion\n",
        "        assert stage_tracking['current_stage'] == 4\n",
        "        assert stage_tracking['completed_stages'] == [1, 2, 3, 4]\n",
        "        \n",
        "        # Validate final output exists and is correct\n",
        "        final_output_path = os.path.join(p.stage_4_final_output_dir, 'project_final_output.txt')\n",
        "        assert os.path.exists(final_output_path)\n",
        "        \n",
        "        with open(final_output_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            assert \"Project completed successfully\" in content\n",
        "            assert \"[1, 2, 3, 4]\" in content\n",
        "            assert \"Final stage: 4\" in content\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_error_handling_in_complete_project_workflow()\n",
        "\n",
        "Test error handling and recovery in complete project workflows.\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    def test_error_handling_in_complete_project_workflow(self, temp_dir):\n",
        "        \"\"\"Test error handling and recovery in complete project workflows.\"\"\"\n",
        "        error_tracking = {'errors_encountered': [], 'recovery_successful': False}\n",
        "        \n",
        "        project_dir = os.path.join(temp_dir, 'error_handling_project')\n",
        "        p = hb.ProjectFlow(project_dir)\n",
        "        \n",
        "        def reliable_setup_task(p):\n",
        "            \"\"\"Task that always succeeds.\"\"\"\n",
        "            setup_file = os.path.join(p.cur_dir, 'setup_success.txt')\n",
        "            with open(setup_file, 'w') as f:\n",
        "                f.write(\"Setup completed successfully\")\n",
        "        \n",
        "        def error_prone_iterator(p):\n",
        "            \"\"\"Iterator that may encounter issues but handles them gracefully.\"\"\"\n",
        "            try:\n",
        "                p.iterator_replacements = {\n",
        "                    'scenario': ['success', 'warning', 'recoverable'],\n",
        "                    'cur_dir_parent_dir': [\n",
        "                        os.path.join(p.cur_dir, 'success_scenario'),\n",
        "                        os.path.join(p.cur_dir, 'warning_scenario'),\n",
        "                        os.path.join(p.cur_dir, 'recoverable_scenario')\n",
        "                    ]\n",
        "                }\n",
        "                \n",
        "                for dir_path in p.iterator_replacements['cur_dir_parent_dir']:\n",
        "                    os.makedirs(dir_path, exist_ok=True)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                error_tracking['errors_encountered'].append(f\"Iterator setup error: {e}\")\n",
        "                # Recovery: set minimal scenarios\n",
        "                p.iterator_replacements = {\n",
        "                    'scenario': ['recovery'],\n",
        "                    'cur_dir_parent_dir': [os.path.join(p.cur_dir, 'recovery_scenario')]\n",
        "                }\n",
        "                os.makedirs(p.iterator_replacements['cur_dir_parent_dir'][0], exist_ok=True)\n",
        "        \n",
        "        def scenario_handling_task(p):\n",
        "            \"\"\"Task that handles different scenario types.\"\"\"\n",
        "            scenario_type = p.scenario\n",
        "            \n",
        "            if scenario_type == 'success':\n",
        "                # Normal processing\n",
        "                output_file = os.path.join(p.cur_dir, 'success_result.txt')\n",
        "                with open(output_file, 'w') as f:\n",
        "                    f.write(\"Successfully processed\")\n",
        "                    \n",
        "            elif scenario_type == 'warning':\n",
        "                # Processing with warnings\n",
        "                error_tracking['errors_encountered'].append(f\"Warning in {scenario_type}\")\n",
        "                output_file = os.path.join(p.cur_dir, 'warning_result.txt')\n",
        "                with open(output_file, 'w') as f:\n",
        "                    f.write(\"Processed with warnings\")\n",
        "                    \n",
        "            elif scenario_type == 'recoverable':\n",
        "                # Recoverable error handling\n",
        "                try:\n",
        "                    # Simulate recoverable error\n",
        "                    if not os.path.exists('/nonexistent/path'):\n",
        "                        raise FileNotFoundError(\"Simulated recoverable error\")\n",
        "                except FileNotFoundError as e:\n",
        "                    error_tracking['errors_encountered'].append(f\"Recoverable error: {e}\")\n",
        "                    # Recovery action\n",
        "                    output_file = os.path.join(p.cur_dir, 'recovered_result.txt')\n",
        "                    with open(output_file, 'w') as f:\n",
        "                        f.write(\"Recovered from error\")\n",
        "                        \n",
        "            elif scenario_type == 'recovery':\n",
        "                # Recovery scenario\n",
        "                error_tracking['recovery_successful'] = True\n",
        "                output_file = os.path.join(p.cur_dir, 'recovery_result.txt')\n",
        "                with open(output_file, 'w') as f:\n",
        "                    f.write(\"Recovery scenario executed\")\n",
        "        \n",
        "        def cleanup_and_summary_task(p):\n",
        "            \"\"\"Final task that summarizes error handling.\"\"\"\n",
        "            summary_file = os.path.join(p.cur_dir, 'error_summary.txt')\n",
        "            with open(summary_file, 'w') as f:\n",
        "                f.write(f\"Errors encountered: {len(error_tracking['errors_encountered'])}\\n\")\n",
        "                f.write(f\"Recovery successful: {error_tracking['recovery_successful']}\\n\")\n",
        "                f.write(\"Error details:\\n\")\n",
        "                for error in error_tracking['errors_encountered']:\n",
        "                    f.write(f\"- {error}\\n\")\n",
        "        \n",
        "        # Build error-handling workflow\n",
        "        setup = p.add_task(reliable_setup_task)\n",
        "        error_iterator = p.add_iterator(error_prone_iterator, run_in_parallel=False)\n",
        "        scenario_task = p.add_task(scenario_handling_task, parent=error_iterator)\n",
        "        cleanup = p.add_task(cleanup_and_summary_task)\n",
        "        \n",
        "        # Execute workflow (should complete despite errors)\n",
        "        p.execute()\n",
        "        \n",
        "        # Validate error handling\n",
        "        assert len(error_tracking['errors_encountered']) > 0  # Should have encountered some issues\n",
        "        \n",
        "        # Validate workflow completed\n",
        "        summary_file = os.path.join(p.cleanup_and_summary_task_dir, 'error_summary.txt')\n",
        "        assert os.path.exists(summary_file)\n",
        "        \n",
        "        # Validate error summary\n",
        "        with open(summary_file, 'r') as f:\n",
        "            content = f.read()\n",
        "            assert \"Errors encountered:\" in content\n",
        "            assert \"Recovery successful:\" in content\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_file_lifecycle_operations_workflow()\n",
        "\n",
        "Test complete file lifecycle operations in project workflow.\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    def test_file_lifecycle_operations_workflow(self, temp_dir):\n",
        "        \"\"\"Test complete file lifecycle operations in project workflow.\"\"\"\n",
        "        file_operations = []\n",
        "        \n",
        "        project_dir = os.path.join(temp_dir, 'file_lifecycle_project')\n",
        "        p = hb.ProjectFlow(project_dir)\n",
        "        \n",
        "        def file_creation_task(p):\n",
        "            \"\"\"Create various types of files.\"\"\"\n",
        "            file_operations.append(\"file_creation_started\")\n",
        "            \n",
        "            # Create different file types\n",
        "            files_to_create = [\n",
        "                ('data.txt', 'Sample text data'),\n",
        "                ('config.json', '{\"setting\": \"value\", \"enabled\": true}'),\n",
        "                ('results.csv', 'name,value,status\\ntest,100,pass')\n",
        "            ]\n",
        "            \n",
        "            for filename, content in files_to_create:\n",
        "                file_path = os.path.join(p.cur_dir, filename)\n",
        "                with open(file_path, 'w') as f:\n",
        "                    f.write(content)\n",
        "                file_operations.append(f\"created_{filename}\")\n",
        "                assert os.path.exists(file_path)\n",
        "        \n",
        "        def file_modification_iterator(p):\n",
        "            \"\"\"Modify files in different ways.\"\"\"\n",
        "            file_operations.append(\"file_modification_started\")\n",
        "            \n",
        "            p.iterator_replacements = {\n",
        "                'modification_type': ['append', 'update', 'backup'],\n",
        "                'cur_dir_parent_dir': [\n",
        "                    os.path.join(p.cur_dir, 'append_mod'),\n",
        "                    os.path.join(p.cur_dir, 'update_mod'),\n",
        "                    os.path.join(p.cur_dir, 'backup_mod')\n",
        "                ]\n",
        "            }\n",
        "            \n",
        "            for dir_path in p.iterator_replacements['cur_dir_parent_dir']:\n",
        "                os.makedirs(dir_path, exist_ok=True)\n",
        "        \n",
        "        def modification_task(p):\n",
        "            \"\"\"Apply specific modification type.\"\"\"\n",
        "            file_operations.append(f\"modification_{p.modification_type}_started\")\n",
        "            \n",
        "            source_dir = p.file_creation_task_dir\n",
        "            \n",
        "            if p.modification_type == 'append':\n",
        "                # Append to existing files\n",
        "                source_file = os.path.join(source_dir, 'data.txt')\n",
        "                if os.path.exists(source_file):\n",
        "                    target_file = os.path.join(p.cur_dir, 'appended_data.txt')\n",
        "                    # Copy original and append\n",
        "                    shutil.copy2(source_file, target_file)\n",
        "                    with open(target_file, 'a') as f:\n",
        "                        f.write(\"\\nAppended content\")\n",
        "                    file_operations.append(\"append_completed\")\n",
        "                    \n",
        "            elif p.modification_type == 'update':\n",
        "                # Update file contents\n",
        "                source_file = os.path.join(source_dir, 'config.json')\n",
        "                if os.path.exists(source_file):\n",
        "                    target_file = os.path.join(p.cur_dir, 'updated_config.json')\n",
        "                    with open(target_file, 'w') as f:\n",
        "                        f.write('{\"setting\": \"updated_value\", \"enabled\": false, \"version\": 2}')\n",
        "                    file_operations.append(\"update_completed\")\n",
        "                    \n",
        "            elif p.modification_type == 'backup':\n",
        "                # Create backup copies\n",
        "                source_files = ['data.txt', 'config.json', 'results.csv']\n",
        "                for filename in source_files:\n",
        "                    source_path = os.path.join(source_dir, filename)\n",
        "                    if os.path.exists(source_path):\n",
        "                        backup_name = f'backup_{filename}'\n",
        "                        backup_path = os.path.join(p.cur_dir, backup_name)\n",
        "                        shutil.copy2(source_path, backup_path)\n",
        "                        file_operations.append(f\"backed_up_{filename}\")\n",
        "        \n",
        "        def file_validation_task(p):\n",
        "            \"\"\"Validate all file operations completed correctly.\"\"\"\n",
        "            file_operations.append(\"validation_started\")\n",
        "            \n",
        "            validation_results = []\n",
        "            \n",
        "            # Check original files\n",
        "            original_files = ['data.txt', 'config.json', 'results.csv']\n",
        "            for filename in original_files:\n",
        "                file_path = os.path.join(p.file_creation_task_dir, filename)\n",
        "                if os.path.exists(file_path):\n",
        "                    validation_results.append(f\"original_{filename}_exists\")\n",
        "            \n",
        "            # Check modified files\n",
        "            append_file = os.path.join(p.file_modification_iterator_dir, 'append_mod', 'appended_data.txt')\n",
        "            update_file = os.path.join(p.file_modification_iterator_dir, 'update_mod', 'updated_config.json')\n",
        "            \n",
        "            if os.path.exists(append_file):\n",
        "                validation_results.append(\"append_file_exists\")\n",
        "            if os.path.exists(update_file):\n",
        "                validation_results.append(\"update_file_exists\")\n",
        "            \n",
        "            # Check backup files\n",
        "            backup_dir = os.path.join(p.file_modification_iterator_dir, 'backup_mod')\n",
        "            backup_files = ['backup_data.txt', 'backup_config.json', 'backup_results.csv']\n",
        "            for backup_file in backup_files:\n",
        "                backup_path = os.path.join(backup_dir, backup_file)\n",
        "                if os.path.exists(backup_path):\n",
        "                    validation_results.append(f\"{backup_file}_exists\")\n",
        "            \n",
        "            # Write validation report\n",
        "            report_file = os.path.join(p.cur_dir, 'validation_report.txt')\n",
        "            with open(report_file, 'w') as f:\n",
        "                f.write(\"File Lifecycle Validation Report\\n\")\n",
        "                f.write(f\"Operations performed: {len(file_operations)}\\n\")\n",
        "                f.write(f\"Validations passed: {len(validation_results)}\\n\")\n",
        "                f.write(\"\\nOperation details:\\n\")\n",
        "                for op in file_operations:\n",
        "                    f.write(f\"- {op}\\n\")\n",
        "                f.write(\"\\nValidation results:\\n\")\n",
        "                for result in validation_results:\n",
        "                    f.write(f\"- {result}\\n\")\n",
        "        \n",
        "        # Build file operations workflow\n",
        "        creation = p.add_task(file_creation_task)\n",
        "        modification_iter = p.add_iterator(file_modification_iterator, run_in_parallel=False)\n",
        "        modification = p.add_task(modification_task, parent=modification_iter)\n",
        "        validation = p.add_task(file_validation_task)\n",
        "        \n",
        "        # Execute file operations workflow\n",
        "        p.execute()\n",
        "        \n",
        "        # Validate workflow completion\n",
        "        assert len(file_operations) >= 10  # Should have many file operations\n",
        "        \n",
        "        # Validate final report exists\n",
        "        report_file = os.path.join(p.file_validation_task_dir, 'validation_report.txt')\n",
        "        assert os.path.exists(report_file)\n",
        "        \n",
        "        # Validate report content\n",
        "        with open(report_file, 'r') as f:\n",
        "            content = f.read()\n",
        "            assert \"File Lifecycle Validation Report\" in content\n",
        "            assert \"Operations performed:\" in content\n",
        "            assert \"Validations passed:\" in content\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Smoke Tests\n",
        "\n",
        "**Source File:** `hazelbean_tests/system/test_smoke.py`\n",
        "\n",
        "### test_hazelbean_imports_successfully()\n",
        "\n",
        "Test that hazelbean can be imported without errors\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_hazelbean_imports_successfully(self):\n",
        "        \"\"\"Test that hazelbean can be imported without errors\"\"\"\n",
        "        # This is already handled by the import above, but let's be explicit\n",
        "        import hazelbean as hb\n",
        "        assert hb is not None\n",
        "        assert hasattr(hb, \"ProjectFlow\")\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_projectflow_imports()\n",
        "\n",
        "Test that ProjectFlow is available and can be imported\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_projectflow_imports(self):\n",
        "        \"\"\"Test that ProjectFlow is available and can be imported\"\"\"\n",
        "        assert hasattr(hb, \"ProjectFlow\")\n",
        "        \n",
        "        # Test that we can instantiate ProjectFlow\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            p = hb.ProjectFlow(temp_dir)\n",
        "            assert p is not None\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_hazelbean_import_performance()\n",
        "\n",
        "Benchmark the import time of hazelbean module.\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    @pytest.mark.benchmark\n",
        "    def test_hazelbean_import_performance(self, benchmark):\n",
        "        \"\"\"Benchmark the import time of hazelbean module.\"\"\"\n",
        "        def import_hazelbean():\n",
        "            import hazelbean as hb\n",
        "            return hb\n",
        "        \n",
        "        result = benchmark(import_hazelbean)\n",
        "        assert hasattr(result, \"ProjectFlow\")\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_projectflow_basic_functionality()\n",
        "\n",
        "Test basic ProjectFlow functionality works\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_projectflow_basic_functionality(self):\n",
        "        \"\"\"Test basic ProjectFlow functionality works\"\"\"\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            p = hb.ProjectFlow(temp_dir)\n",
        "            \n",
        "            # Test basic get_path functionality with an existing file\n",
        "            test_file = os.path.join(temp_dir, \"test_file.txt\")\n",
        "            with open(test_file, 'w') as f:\n",
        "                f.write(\"test content\")\n",
        "            \n",
        "            path = p.get_path(\"test_file.txt\")\n",
        "            assert path is not None\n",
        "            assert \"test_file.txt\" in path\n",
        "            assert os.path.exists(path)\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_common_hazelbean_functions_available()\n",
        "\n",
        "Test that common hazelbean functions are available\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_common_hazelbean_functions_available(self):\n",
        "        \"\"\"Test that common hazelbean functions are available\"\"\"\n",
        "        # Test that key functions are available\n",
        "        assert hasattr(hb, \"temp\")\n",
        "        assert hasattr(hb, \"get_path\") \n",
        "        assert hasattr(hb, \"describe\")\n",
        "        assert hasattr(hb, \"save_array_as_npy\")\n",
        "        \n",
        "        # Test that we can call temp function\n",
        "        temp_path = hb.temp('.txt', remove_at_exit=True)\n",
        "        assert temp_path is not None\n",
        "        assert temp_path.endswith('.txt')\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_numpy_integration()\n",
        "\n",
        "Test basic numpy integration with hazelbean\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_numpy_integration(self):\n",
        "        \"\"\"Test basic numpy integration with hazelbean\"\"\"\n",
        "        import numpy as np\n",
        "        \n",
        "        # Create test array\n",
        "        test_array = np.random.rand(10, 10)\n",
        "        \n",
        "        # Test saving with hazelbean\n",
        "        temp_path = hb.temp('.npy', remove_at_exit=True)\n",
        "        hb.save_array_as_npy(test_array, temp_path)\n",
        "        \n",
        "        # Verify file was created\n",
        "        assert os.path.exists(temp_path)\n",
        "        \n",
        "        # Test describe function\n",
        "        result = hb.describe(temp_path, surpress_print=True, surpress_logger=True)\n",
        "        assert result is not None\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_basic_error_handling()\n",
        "\n",
        "Test that get_path raises NameError for unresolvable paths.\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_basic_error_handling(self):\n",
        "        \"\"\"Test that get_path raises NameError for unresolvable paths.\n",
        "        \n",
        "        Note: get_path() intentionally raises NameError (not FileNotFoundError) because\n",
        "        it performs complex path resolution logic beyond simple file existence checking:\n",
        "        - Resolves paths relative to project structure\n",
        "        - Searches multiple possible_dirs\n",
        "        - Attempts cloud bucket downloads\n",
        "        \n",
        "        NameError semantically indicates \"name/reference resolution failed\" which is\n",
        "        more accurate than \"file at specific path not found\".\n",
        "        See docs/plans/exception-handling-analysis.md for detailed rationale.\n",
        "        \"\"\"\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            p = hb.ProjectFlow(temp_dir)\n",
        "            \n",
        "            # get_path should raise NameError when path cannot be resolved\n",
        "            with pytest.raises(NameError) as exc_info:\n",
        "                path = p.get_path(\"definitely_does_not_exist.txt\")\n",
        "            \n",
        "            # Verify error message provides useful context\n",
        "            error_msg = str(exc_info.value)\n",
        "            assert \"does not exist\" in error_msg.lower()\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_get_path_generates_doc()\n",
        "\n",
        "Smoke-test + write example QMD.\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    def test_get_path_generates_doc(self, tmp_path):\n",
        "        \"\"\"Smoke-test + write example QMD.\"\"\"\n",
        "        # ---------- Test behaviour -------------------------------------------\n",
        "        p = hb.ProjectFlow(project_dir=str(tmp_path))     # cast Path → str\n",
        "        \n",
        "        # Create the file so get_path can find it\n",
        "        test_file = tmp_path / \"foo.txt\"\n",
        "        test_file.write_text(\"test content\")\n",
        "        \n",
        "        resolved = p.get_path(\"foo.txt\")\n",
        "\n",
        "        assert resolved.endswith(\"foo.txt\")               # file name correct\n",
        "        assert str(tmp_path) in resolved                  # lives in project dir\n",
        "\n",
        "        # ---------- Generate documentation -----------------------------------\n",
        "        qmd = DOCS_DIR / \"get_path_example.qmd\"\n",
        "\n",
        "        qmd.write_text(textwrap.dedent(f\"\"\"\n",
        "        ---\n",
        "        title: \"Hazelbean example – get_path\"\n",
        "        execute: true          # run the chunk when rendering\n",
        "        freeze: auto\n",
        "        ---\n",
        "\n",
        "        ```{{python}}\n",
        "        import hazelbean as hb, tempfile, os\n",
        "        with tempfile.TemporaryDirectory() as tmp:\n",
        "            p = hb.ProjectFlow(project_dir=tmp)\n",
        "            print(p.get_path(\"foo.txt\"))\n",
        "        ```\n",
        "\n",
        "        Above we create a throw-away project directory and ask Hazelbean for\n",
        "        `\"foo.txt\"`.  The printed path shows how *get_path* resolves files relative\n",
        "        to the project workspace.\n",
        "        \"\"\"))\n",
        "        \n",
        "        # Verify documentation was generated\n",
        "        assert qmd.exists()\n",
        "        assert qmd.stat().st_size > 0  # File has content\n",
        "        \n",
        "        # Verify content contains expected elements\n",
        "        content = qmd.read_text()\n",
        "        assert \"get_path\" in content\n",
        "        assert \"hazelbean\" in content\n",
        "        assert \"ProjectFlow\" in content\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_error_handling_documentation()\n",
        "\n",
        "Test documentation generation for error handling scenarios\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_error_handling_documentation(self):\n",
        "        \"\"\"Test documentation generation for error handling scenarios\"\"\"\n",
        "        qmd = DOCS_DIR / \"get_path_error_handling.qmd\"\n",
        "\n",
        "        qmd.write_text(textwrap.dedent(\"\"\"\n",
        "        ---\n",
        "        title: \"Hazelbean – get_path Error Handling\"\n",
        "        execute: true\n",
        "        freeze: auto\n",
        "        ---\n",
        "\n",
        "        ## Error Handling Examples"
      ],
      "id": "a2709552"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "        "
      },
      "source": [
        "import hazelbean as hb, tempfile\n",
        "\n",
        "# Test with non-existent file\n",
        "with tempfile.TemporaryDirectory() as tmp:\n",
        "    p = hb.ProjectFlow(project_dir=tmp)\n",
        "    \n",
        "    # This should work even if file doesn't exist\n",
        "    try:\n",
        "        path = p.get_path(\"missing_file.txt\")\n",
        "        print(f\"Resolved path: {path}\")\n",
        "        print(\"get_path handles missing files gracefully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")"
      ],
      "id": "338f18a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        The above demonstrates how `get_path` handles missing files.\n",
        "        \"\"\"))\n",
        "        \n",
        "        # Verify documentation was generated\n",
        "        assert qmd.exists()\n",
        "        assert qmd.stat().st_size > 0\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_performance_documentation()\n",
        "\n",
        "Test documentation generation for performance examples\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_performance_documentation(self):\n",
        "        \"\"\"Test documentation generation for performance examples\"\"\"\n",
        "        qmd = DOCS_DIR / \"get_path_performance_guide.qmd\"\n",
        "\n",
        "        qmd.write_text(textwrap.dedent(\"\"\"\n",
        "        ---\n",
        "        title: \"Hazelbean – get_path Performance Guide\"\n",
        "        execute: true\n",
        "        freeze: auto\n",
        "        ---\n",
        "\n",
        "        ## Performance Characteristics"
      ],
      "id": "f2653421"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "        "
      },
      "source": [
        "import hazelbean as hb, tempfile, time\n",
        "\n",
        "with tempfile.TemporaryDirectory() as tmp:\n",
        "    p = hb.ProjectFlow(project_dir=tmp)\n",
        "    \n",
        "    # Create a test file\n",
        "    test_file = \"performance_test.txt\"\n",
        "    with open(f\"{tmp}/{test_file}\", 'w') as f:\n",
        "        f.write(\"test content\")\n",
        "    \n",
        "    # Benchmark single call\n",
        "    start = time.time()\n",
        "    path = p.get_path(test_file)\n",
        "    single_call_time = time.time() - start\n",
        "    \n",
        "    print(f\"Single get_path call: {single_call_time:.6f} seconds\")\n",
        "    print(f\"Resolved path: {path}\")\n",
        "    \n",
        "    # Benchmark multiple calls\n",
        "    start = time.time()\n",
        "    for i in range(100):\n",
        "        path = p.get_path(test_file)\n",
        "    multiple_calls_time = time.time() - start\n",
        "    \n",
        "    print(f\"100 get_path calls: {multiple_calls_time:.6f} seconds\")\n",
        "    print(f\"Average per call: {multiple_calls_time/100:.6f} seconds\")"
      ],
      "id": "827ede90",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        This shows typical performance characteristics of the `get_path` method.\n",
        "        \"\"\"))\n",
        "        \n",
        "        # Verify documentation was generated\n",
        "        assert qmd.exists()\n",
        "        assert qmd.stat().st_size > 0\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_file_formats_documentation()\n",
        "\n",
        "Test documentation generation for different file formats\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_file_formats_documentation(self):\n",
        "        \"\"\"Test documentation generation for different file formats\"\"\"\n",
        "        qmd = DOCS_DIR / \"get_path_file_formats.qmd\"\n",
        "\n",
        "        qmd.write_text(textwrap.dedent(\"\"\"\n",
        "        ---\n",
        "        title: \"Hazelbean – get_path File Format Support\"\n",
        "        execute: true\n",
        "        freeze: auto\n",
        "        ---\n",
        "\n",
        "        ## Supported File Formats"
      ],
      "id": "12faa9b5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "        "
      },
      "source": [
        "import hazelbean as hb, tempfile, os\n",
        "\n",
        "with tempfile.TemporaryDirectory() as tmp:\n",
        "    p = hb.ProjectFlow(project_dir=tmp)\n",
        "    \n",
        "    # Test different file extensions\n",
        "    file_types = [\n",
        "        \"data.csv\",\n",
        "        \"raster.tif\", \n",
        "        \"vector.shp\",\n",
        "        \"config.json\",\n",
        "        \"script.py\",\n",
        "        \"document.txt\"\n",
        "    ]\n",
        "    \n",
        "    print(\"Testing file format resolution:\")\n",
        "    for file_type in file_types:\n",
        "        path = p.get_path(file_type)\n",
        "        print(f\"  {file_type} -> {os.path.basename(path)}\")"
      ],
      "id": "d0deb731",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        The `get_path` method works with various file formats commonly used\n",
        "        in geospatial and scientific computing workflows.\n",
        "        \"\"\"))\n",
        "        \n",
        "        # Verify documentation was generated\n",
        "        assert qmd.exists()\n",
        "        assert qmd.stat().st_size > 0\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_temp_directory_creation()\n",
        "\n",
        "Test that temporary directory creation works\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_temp_directory_creation(self):\n",
        "        \"\"\"Test that temporary directory creation works\"\"\"\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            p = hb.ProjectFlow(temp_dir)\n",
        "            \n",
        "            # Should be able to create subdirectories\n",
        "            sub_path = os.path.join(temp_dir, \"subdir\", \"nested\")\n",
        "            os.makedirs(sub_path, exist_ok=True)\n",
        "            \n",
        "            assert os.path.exists(sub_path)\n",
        "            \n",
        "            # get_path should construct paths without validation when raise_error_if_fail=False\n",
        "            nested_file_path = p.get_path(\"subdir/nested/test.txt\", raise_error_if_fail=False)\n",
        "            assert \"nested\" in nested_file_path\n",
        "            assert \"test.txt\" in nested_file_path\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_multiple_projectflow_instances()\n",
        "\n",
        "Test that multiple ProjectFlow instances can coexist\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_multiple_projectflow_instances(self):\n",
        "        \"\"\"Test that multiple ProjectFlow instances can coexist\"\"\"\n",
        "        with tempfile.TemporaryDirectory() as temp_dir1:\n",
        "            with tempfile.TemporaryDirectory() as temp_dir2:\n",
        "                p1 = hb.ProjectFlow(temp_dir1)\n",
        "                p2 = hb.ProjectFlow(temp_dir2)\n",
        "                \n",
        "                # Each should resolve to its own directory (without validation)\n",
        "                path1 = p1.get_path(\"test.txt\", raise_error_if_fail=False)\n",
        "                path2 = p2.get_path(\"test.txt\", raise_error_if_fail=False)\n",
        "                \n",
        "                assert temp_dir1 in path1\n",
        "                assert temp_dir2 in path2\n",
        "                assert path1 != path2\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_relative_vs_absolute_paths()\n",
        "\n",
        "Test handling of relative vs absolute paths with actual files\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_relative_vs_absolute_paths(self):\n",
        "        \"\"\"Test handling of relative vs absolute paths with actual files\"\"\"\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            p = hb.ProjectFlow(temp_dir)\n",
        "            \n",
        "            # Create test file for relative path testing\n",
        "            rel_file = os.path.join(temp_dir, \"relative_file.txt\")\n",
        "            with open(rel_file, 'w') as f:\n",
        "                f.write(\"relative test content\")\n",
        "            \n",
        "            # Test relative path\n",
        "            rel_path = p.get_path(\"relative_file.txt\")\n",
        "            assert \"relative_file.txt\" in rel_path\n",
        "            assert os.path.exists(rel_path)\n",
        "            \n",
        "            # Create test file for absolute path testing\n",
        "            abs_file = os.path.join(temp_dir, \"absolute_file.txt\")\n",
        "            with open(abs_file, 'w') as f:\n",
        "                f.write(\"absolute test content\")\n",
        "            \n",
        "            # Test absolute path\n",
        "            abs_input = os.path.abspath(abs_file)\n",
        "            abs_path = p.get_path(abs_input)\n",
        "            assert \"absolute_file.txt\" in abs_path\n",
        "            assert os.path.exists(abs_path)\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_special_characters_in_paths()\n",
        "\n",
        "Test handling of special characters in file paths with actual files\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_special_characters_in_paths(self):\n",
        "        \"\"\"Test handling of special characters in file paths with actual files\"\"\"\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            p = hb.ProjectFlow(temp_dir)\n",
        "            \n",
        "            # Test various special characters (that are valid in file names)\n",
        "            special_files = [\n",
        "                \"file_with_underscores.txt\",\n",
        "                \"file-with-hyphens.txt\",\n",
        "                \"file.with.dots.txt\",\n",
        "                \"file with spaces.txt\",  # May not be supported on all systems\n",
        "                \"file123numbers.txt\",\n",
        "                \"UPPERCASE.TXT\",\n",
        "                \"mixedCase.TxT\"\n",
        "            ]\n",
        "            \n",
        "            for special_file in special_files:\n",
        "                # Create file with special characters\n",
        "                file_path = os.path.join(temp_dir, special_file)\n",
        "                try:\n",
        "                    with open(file_path, 'w') as f:\n",
        "                        f.write(f\"test content for {special_file}\")\n",
        "                    \n",
        "                    # Test get_path with actual file\n",
        "                    path = p.get_path(special_file)\n",
        "                    assert special_file in path or os.path.basename(path) == special_file\n",
        "                    assert os.path.exists(path)\n",
        "                except Exception as e:\n",
        "                    # Some special characters might not be supported on some systems\n",
        "                    # NameError indicates path resolution failure (hazelbean's design)\n",
        "                    assert isinstance(e, (ValueError, OSError, NameError))\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_concurrent_access()\n",
        "\n",
        "Test basic concurrent access patterns with actual files\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_concurrent_access(self):\n",
        "        \"\"\"Test basic concurrent access patterns with actual files\"\"\"\n",
        "        import threading\n",
        "        import time\n",
        "        \n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            p = hb.ProjectFlow(temp_dir)\n",
        "            \n",
        "            results = []\n",
        "            errors = []\n",
        "            \n",
        "            # Pre-create all files before threads start (avoid race conditions)\n",
        "            for thread_id in range(3):\n",
        "                for i in range(10):\n",
        "                    file_path = os.path.join(temp_dir, f\"thread_{thread_id}_file_{i}.txt\")\n",
        "                    with open(file_path, 'w') as f:\n",
        "                        f.write(f\"Thread {thread_id} file {i}\")\n",
        "            \n",
        "            def worker_thread(thread_id):\n",
        "                try:\n",
        "                    for i in range(10):\n",
        "                        path = p.get_path(f\"thread_{thread_id}_file_{i}.txt\")\n",
        "                        results.append((thread_id, path))\n",
        "                        time.sleep(0.001)  # Small delay\n",
        "                except Exception as e:\n",
        "                    errors.append((thread_id, e))\n",
        "            \n",
        "            # Create and start multiple threads\n",
        "            threads = []\n",
        "            for i in range(3):\n",
        "                t = threading.Thread(target=worker_thread, args=(i,))\n",
        "                threads.append(t)\n",
        "                t.start()\n",
        "            \n",
        "            # Wait for all threads to complete\n",
        "            for t in threads:\n",
        "                t.join()\n",
        "            \n",
        "            # Verify results\n",
        "            assert len(errors) == 0, f\"Errors in concurrent access: {errors}\"\n",
        "            assert len(results) == 30, f\"Expected 30 results, got {len(results)}\"\n",
        "            \n",
        "            # Verify all paths exist and are unique per thread\n",
        "            thread_paths = {}\n",
        "            for thread_id, path in results:\n",
        "                assert os.path.exists(path), f\"Path {path} doesn't exist\"\n",
        "                if thread_id not in thread_paths:\n",
        "                    thread_paths[thread_id] = []\n",
        "                thread_paths[thread_id].append(path)\n",
        "            \n",
        "            assert len(thread_paths) == 3, \"Should have results from all 3 threads\"\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_python_version_compatibility()\n",
        "\n",
        "Test Python version compatibility with actual file\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_python_version_compatibility(self):\n",
        "        \"\"\"Test Python version compatibility with actual file\"\"\"\n",
        "        import sys\n",
        "        \n",
        "        # Should work with Python 3.7+\n",
        "        assert sys.version_info >= (3, 7), f\"Python version {sys.version_info} may not be supported\"\n",
        "        \n",
        "        # Test that hazelbean works with current Python version\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            p = hb.ProjectFlow(temp_dir)\n",
        "            \n",
        "            # Create test file\n",
        "            test_file = os.path.join(temp_dir, \"test.txt\")\n",
        "            with open(test_file, 'w') as f:\n",
        "                f.write(\"Python version compatibility test\")\n",
        "            \n",
        "            # Test basic get_path functionality\n",
        "            path = p.get_path(\"test.txt\")\n",
        "            assert path is not None\n",
        "            assert os.path.exists(path)\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_required_dependencies_available()\n",
        "\n",
        "Test that required dependencies are available\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_required_dependencies_available(self):\n",
        "        \"\"\"Test that required dependencies are available\"\"\"\n",
        "        # Test numpy\n",
        "        import numpy as np\n",
        "        assert hasattr(np, 'array')\n",
        "        \n",
        "        # Test that hazelbean can use numpy\n",
        "        arr = np.random.rand(5, 5)\n",
        "        temp_path = hb.temp('.npy', remove_at_exit=True)\n",
        "        hb.save_array_as_npy(arr, temp_path)\n",
        "        assert os.path.exists(temp_path)\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "### test_file_system_permissions()\n",
        "\n",
        "Test basic file system permissions\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Source code\n",
        "\n",
        "```python\n",
        "    @pytest.mark.smoke\n",
        "    def test_file_system_permissions(self):\n",
        "        \"\"\"Test basic file system permissions\"\"\"\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            # Test directory creation\n",
        "            test_dir = os.path.join(temp_dir, \"permission_test\")\n",
        "            os.makedirs(test_dir)\n",
        "            assert os.path.exists(test_dir)\n",
        "            \n",
        "            # Test file creation\n",
        "            test_file = os.path.join(test_dir, \"test.txt\")\n",
        "            with open(test_file, 'w') as f:\n",
        "                f.write(\"permission test\")\n",
        "            assert os.path.exists(test_file)\n",
        "            \n",
        "            # Test file reading\n",
        "            with open(test_file, 'r') as f:\n",
        "                content = f.read()\n",
        "            assert content == \"permission test\"\n",
        "            \n",
        "            # Test hazelbean can work in this environment\n",
        "            p = hb.ProjectFlow(temp_dir)\n",
        "            path = p.get_path(\"permission_test/test.txt\")\n",
        "            assert \"test.txt\" in path\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Running System Tests\n",
        "\n",
        "To run these tests:\n",
        "\n",
        "```{.bash}\n",
        "# Activate the hazelbean environment\n",
        "conda activate hazelbean_env\n",
        "\n",
        "# Run all system tests\n",
        "pytest hazelbean_tests/system/ -v\n",
        "\n",
        "# Run specific test file\n",
        "pytest hazelbean_tests/system/<test_file>.py -v\n",
        "\n",
        "# Run with coverage\n",
        "pytest hazelbean_tests/system/ --cov=hazelbean --cov-report=html\n",
        "```\n",
        "\n",
        "## Test Organization\n",
        "\n",
        "Tests are organized by:\n",
        "- **Test Files** - Each file tests a specific module or feature\n",
        "- **Test Functions** - Individual test cases within files\n",
        "- **Test Classes** - Grouped related tests (where applicable)\n",
        "- **Fixtures** - Shared test setup and teardown\n",
        "\n",
        "## Related Documentation\n",
        "\n",
        "- [Test Strategy](../README.md) - Overall testing approach\n",
        "- [Contributing](../../CONTRIBUTING.md) - How to write tests\n",
        "- [CI/CD](../../.github/workflows/) - Automated testing"
      ],
      "id": "05b76d82"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/homebrew/Caskroom/mambaforge/base/envs/hazelbean_env/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}