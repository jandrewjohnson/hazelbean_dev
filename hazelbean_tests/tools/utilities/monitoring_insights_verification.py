"""
Story 10: Monitoring & Metrics - Actionable Insights Verification

This script demonstrates that the monitoring system provides actionable insights
for system optimization and quality improvement by showcasing real-world scenarios
and the insights generated by each monitoring component.

Story 10: Task 10.6 - Verification Script
"""

import os
import sys
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
import json

# Add the project root to path for imports
sys.path.append(str(Path(__file__).parent.parent))

# Import monitoring components
from hazelbean_tests.qmd_automation.monitoring import (
    MetricsCollector, QualityReporter, PerformanceMonitor,
    ErrorTracker, AlertSystem, MetricsDashboard
)
from hazelbean_tests.qmd_automation.core.quality_assessment_engine import (
    QualityCategory, QualityAssessment
)


def create_sample_system_data(metrics_dir):
    """Create realistic sample data simulating various system scenarios"""
    
    collector = MetricsCollector(metrics_dir)
    performance_monitor = PerformanceMonitor(collector)
    error_tracker = ErrorTracker(metrics_dir / 'errors')
    
    print("üìä Creating sample system data...")
    
    # Scenario 1: Declining performance over time
    base_time = datetime.now() - timedelta(days=7)
    for i in range(7):
        generation_result = {
            'timestamp': base_time + timedelta(days=i),
            'total_tests_processed': 15,
            'successful_generations': max(10, 15 - i),  # Declining success
            'failed_generations': min(5, i),
            'generation_time_seconds': 3.0 + (i * 0.8),  # Increasing time
            'quality_distribution': {
                QualityCategory.HIGH: max(2, 8 - i),
                QualityCategory.MEDIUM: 6,
                QualityCategory.LOW: min(5, 2 + i),  # Increasing low quality
                QualityCategory.STUB: min(2, i // 2)
            },
            'template_usage': {
                'unit_test.qmd.j2': 10,
                'integration_test.qmd.j2': 3,
                'performance_test.qmd.j2': 2
            },
            'plugin_execution_times': {
                'code_simplifier': 1.2 + (i * 0.2),  # Increasing time
                'import_optimizer': 0.8 + (i * 0.1),
                'data_path_resolver': 2.1 + (i * 0.3)
            },
            'errors': [f'File processing error on day {i}'] if i > 3 else [],
            'warnings': [f'Deprecated usage warning {i}', f'Performance warning {i}'],
            'processing_mode': 'incremental'
        }
        
        collector.record_generation_run(generation_result)
        
        # Record some performance data
        with performance_monitor.track_operation(f'day_{i}_generation'):
            # Simulate template processing time
            performance_monitor.record_timing('template_processing', 2.5 + (i * 0.5), {
                'template_count': 15,
                'complexity': 'medium' if i < 4 else 'high'
            })
    
    # Quality assessments with declining trend
    quality_assessments = []
    for i in range(5):
        base_score = max(40, 85 - (i * 8))  # Declining quality
        assessment = QualityAssessment(
            quality_score=base_score,
            category=QualityCategory.HIGH if base_score > 80 else 
                    QualityCategory.MEDIUM if base_score > 60 else QualityCategory.LOW,
            educational_value=max(4, 9 - i),
            suggestions=[
                f'Improve test documentation (assessment {i})',
                f'Add more comprehensive assertions (assessment {i})'
            ]
        )
        quality_assessments.append(assessment)
    
    collector.record_quality_assessment(quality_assessments)
    
    # Record some errors for pattern analysis
    try:
        raise FileNotFoundError("Template file not found: advanced_template.qmd.j2")
    except FileNotFoundError as e:
        error_tracker.record_error(e, {
            'operation': 'template_processing',
            'template_name': 'advanced_template.qmd.j2',
            'processing_mode': 'full'
        })
    
    try:
        raise ValueError("Invalid test configuration: missing required fields")
    except ValueError as e:
        error_tracker.record_error(e, {
            'operation': 'test_analysis',
            'file_path': '/path/to/test_invalid_config.py',
            'config_section': 'test_metadata'
        })
    
    # Record duplicate error for pattern detection
    try:
        raise FileNotFoundError("Template file not found: advanced_template.qmd.j2")
    except FileNotFoundError as e:
        error_tracker.record_error(e, {
            'operation': 'template_processing',
            'template_name': 'advanced_template.qmd.j2',
            'processing_mode': 'incremental'
        })
    
    print("‚úÖ Sample data created successfully")
    return collector, performance_monitor, error_tracker


def demonstrate_metrics_insights():
    """Demonstrate actionable insights from each monitoring component"""
    
    print("üîç STORY 10: MONITORING & METRICS - ACTIONABLE INSIGHTS VERIFICATION")
    print("=" * 80)
    
    with tempfile.TemporaryDirectory() as temp_dir:
        metrics_dir = Path(temp_dir)
        
        # Create sample data
        collector, perf_monitor, error_tracker = create_sample_system_data(metrics_dir)
        
        # Initialize other components
        quality_reporter = QualityReporter(metrics_dir)
        alert_config = {
            'performance_threshold': 5.0,
            'failure_rate_threshold': 0.20,
            'quality_decline_threshold': -10.0
        }
        alert_system = AlertSystem(alert_config)
        dashboard = MetricsDashboard(metrics_dir)
        
        print("\n1Ô∏è‚É£  METRICS COLLECTOR INSIGHTS")
        print("-" * 40)
        
        # Get recent metrics and show insights
        recent_metrics = collector.get_recent_metrics(days=7)
        summary_stats = collector.get_summary_stats(days=7)
        
        print(f"üìà Performance Trend Analysis:")
        print(f"   ‚Ä¢ Total generation runs: {summary_stats['total_runs']}")
        print(f"   ‚Ä¢ Success rate: {summary_stats['success_rate']:.1f}%")
        print(f"   ‚Ä¢ Average generation time: {summary_stats['average_generation_time']:.1f}s")
        
        if summary_stats['success_rate'] < 80:
            print("   üö® ACTIONABLE INSIGHT: Low success rate detected!")
            print("      ‚Üí Investigate recent test file changes")
            print("      ‚Üí Review error logs for common failure patterns")
            print("      ‚Üí Consider reverting problematic changes")
        
        if summary_stats['average_generation_time'] > 4:
            print("   üö® ACTIONABLE INSIGHT: Generation time increasing!")
            print("      ‚Üí Profile template processing performance")
            print("      ‚Üí Consider template caching optimizations")
            print("      ‚Üí Review plugin execution times")
        
        print("\n2Ô∏è‚É£  QUALITY REPORTER INSIGHTS")
        print("-" * 40)
        
        # Generate quality report and insights
        quality_report = quality_reporter.generate_quality_report()
        improvement_opportunities = quality_reporter.identify_improvement_opportunities()
        
        print(f"üìä Quality Assessment Summary:")
        print(f"   ‚Ä¢ Total test files analyzed: {quality_report.total_test_files}")
        print(f"   ‚Ä¢ Average quality score: {quality_report.average_quality_score:.1f}")
        
        quality_dist = quality_report.quality_distribution
        for category, count in quality_dist.items():
            if count > 0:
                percentage = (count / quality_report.total_test_files) * 100
                print(f"   ‚Ä¢ {category.value}: {count} files ({percentage:.1f}%)")
        
        print(f"\nüîß Improvement Opportunities ({len(improvement_opportunities)} identified):")
        for i, opportunity in enumerate(improvement_opportunities[:3], 1):
            print(f"   {i}. {opportunity}")
        
        print("\n3Ô∏è‚É£  PERFORMANCE MONITOR INSIGHTS")
        print("-" * 40)
        
        # Analyze performance bottlenecks
        bottlenecks = perf_monitor.analyze_bottlenecks()
        perf_summary = perf_monitor.get_performance_summary()
        
        print(f"‚ö° Performance Analysis:")
        print(f"   ‚Ä¢ Total operations tracked: {perf_summary.total_operations}")
        print(f"   ‚Ä¢ Average operation duration: {perf_summary.average_duration:.2f}s")
        
        if bottlenecks:
            print(f"\nüêå Performance Bottlenecks Identified ({len(bottlenecks)}):")
            for bottleneck in bottlenecks[:3]:
                print(f"   ‚Ä¢ {bottleneck.operation_name}: {bottleneck.average_duration:.2f}s avg ({bottleneck.severity})")
                if bottleneck.recommendations:
                    print(f"     üí° {bottleneck.recommendations[0]}")
        
        # Show operation-specific insights
        template_stats = perf_monitor.get_operation_stats('template_processing', days=7)
        if template_stats:
            print(f"\nüìã Template Processing Analysis:")
            print(f"   ‚Ä¢ Average time: {template_stats['average']:.2f}s")
            print(f"   ‚Ä¢ Operations count: {template_stats['count']}")
            if template_stats['average'] > 3.0:
                print("   üö® ACTIONABLE INSIGHT: Template processing is slow!")
                print("      ‚Üí Consider template pre-compilation")
                print("      ‚Üí Implement template result caching")
                print("      ‚Üí Profile template complexity")
        
        print("\n4Ô∏è‚É£  ERROR TRACKER INSIGHTS")
        print("-" * 40)
        
        # Analyze error patterns
        error_patterns = error_tracker.analyze_error_patterns()
        debugging_report = error_tracker.generate_debugging_report(timeframe_hours=168)  # 1 week
        
        print(f"üêõ Error Analysis:")
        print(f"   ‚Ä¢ Total errors in last week: {debugging_report.error_count}")
        print(f"   ‚Ä¢ Unique error types: {len(debugging_report.error_types)}")
        
        if error_patterns:
            print(f"\nüîç Error Patterns Identified ({len(error_patterns)}):")
            for pattern in error_patterns[:2]:
                print(f"   ‚Ä¢ {pattern.error_type}: {pattern.occurrence_count} occurrences")
                print(f"     Pattern: {pattern.error_message_pattern}")
                if pattern.suggested_fixes:
                    print(f"     üí° {pattern.suggested_fixes[0]}")
        
        if debugging_report.recommendations:
            print(f"\nüîß Debug Recommendations:")
            for rec in debugging_report.recommendations[:2]:
                print(f"   ‚Ä¢ {rec}")
        
        print("\n5Ô∏è‚É£  ALERT SYSTEM INSIGHTS")
        print("-" * 40)
        
        # Check for alerts with current metrics
        if recent_metrics:
            latest_metrics = recent_metrics[-1]
            alerts = alert_system.check_alert_conditions(latest_metrics)
            
            print(f"üö® Alert Analysis:")
            if alerts:
                print(f"   ‚Ä¢ Active alerts: {len(alerts)}")
                for alert in alerts[:3]:
                    print(f"   ‚Ä¢ {alert.severity.value.upper()}: {alert.message}")
                    if alert.metric_value and alert.threshold_value:
                        print(f"     Current: {alert.metric_value:.1f}, Threshold: {alert.threshold_value:.1f}")
            else:
                print("   ‚Ä¢ No active alerts - system performance within thresholds")
            
            # Get alert summary
            alert_summary = alert_system.get_alert_summary(hours=168)
            print(f"   ‚Ä¢ Recent alerts (7 days): {alert_summary['total_alerts']}")
        
        print("\n6Ô∏è‚É£  METRICS DASHBOARD INSIGHTS")
        print("-" * 40)
        
        # Generate dashboard reports
        summary_report = dashboard.generate_summary_report(days=7)
        status_overview = dashboard.generate_status_overview()
        
        print(f"üìä System Overview:")
        print(f"   ‚Ä¢ System status: {status_overview['system_status'].upper()}")
        print(f"   ‚Ä¢ Recent activity: {status_overview['recent_activity']['runs_last_24h']} runs (24h)")
        print(f"   ‚Ä¢ Success rate: {status_overview['recent_activity']['avg_success_rate']:.1f}%")
        
        # Export data for external analysis
        json_export = dashboard.export_metrics_json(timeframe_days=7)
        export_file = metrics_dir / 'metrics_export.json'
        with open(export_file, 'w') as f:
            json.dump(json_export, f, indent=2, default=str)
        
        print(f"   ‚Ä¢ Metrics exported to: {export_file}")
        
        print("\n7Ô∏è‚É£  COMPREHENSIVE SYSTEM RECOMMENDATIONS")
        print("-" * 50)
        
        print("Based on the monitoring analysis, here are the key actionable insights:")
        print()
        
        # Performance recommendations
        avg_time = summary_stats.get('average_generation_time', 0)
        success_rate = summary_stats.get('success_rate', 100)
        
        if avg_time > 4:
            print("üöÄ PERFORMANCE OPTIMIZATION NEEDED:")
            print("   1. Template processing is the primary bottleneck")
            print("   2. Implement template result caching")
            print("   3. Consider parallel template processing")
            print("   4. Profile and optimize plugin execution times")
            print()
        
        if success_rate < 85:
            print("üéØ QUALITY IMPROVEMENT REQUIRED:")
            print("   1. High failure rate indicates systematic issues")
            print("   2. Review recent test file modifications")
            print("   3. Strengthen input validation and error handling")
            print("   4. Implement better test file format validation")
            print()
        
        if error_patterns:
            print("üîß ERROR PATTERN RESOLUTION:")
            print("   1. Template file path issues are recurring")
            print("   2. Implement robust template discovery mechanism")
            print("   3. Add template file existence validation")
            print("   4. Create fallback templates for missing cases")
            print()
        
        print("üìà MONITORING ENHANCEMENTS:")
        print("   1. Set up automated alerting for critical thresholds")
        print("   2. Implement daily quality reports")
        print("   3. Track quality trends over longer periods")
        print("   4. Add user-specific quality metrics")
        print()
        
        print("‚úÖ IMMEDIATE ACTIONS:")
        print("   1. Fix template file path resolution")
        print("   2. Optimize slowest performing operations")
        print("   3. Review and improve low-quality test files")
        print("   4. Set up automated monitoring alerts")
        
        print("\n" + "=" * 80)
        print("üéâ STORY 10 VERIFICATION COMPLETE!")
        print("The monitoring system successfully provides comprehensive,")
        print("actionable insights for system optimization and quality improvement.")
        print("=" * 80)


def run_verification():
    """Run the complete verification of actionable insights"""
    try:
        demonstrate_metrics_insights()
        return True
    except Exception as e:
        print(f"‚ùå Verification failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_verification()
    sys.exit(0 if success else 1)
