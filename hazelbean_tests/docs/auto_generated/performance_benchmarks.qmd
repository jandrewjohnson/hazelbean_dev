---
title: "Hazelbean get_path() - Performance Benchmarks" 
subtitle: "Auto-generated from TestPerformanceBenchmarks test cases"
execute:
  enabled: true
freeze: auto
format:
  html:
    code-fold: false
    toc: true
---

## Overview

This guide demonstrates performance characteristics for Hazelbean's `get_path()` function. Examples are derived from the `TestPerformanceBenchmarks` test suite.

## Setup

```{python}
import hazelbean as hb
import tempfile
import os
import time
from statistics import mean, stdev

# Create a temporary project for demonstration
tmp_dir = tempfile.mkdtemp()
p = hb.ProjectFlow(project_dir=tmp_dir)

# Create test files for benchmarking
test_files = ["test1.txt", "test2.txt", "test3.txt"]
for filename in test_files:
    with open(os.path.join(tmp_dir, filename), 'w') as f:
        f.write("test content")

print(f"Test project created with {len(test_files)} files")
```

## Single Call Performance

Benchmark individual `get_path()` call performance:

```{python}
# Benchmark single get_path call
def benchmark_single_call(project, filename, iterations=50):
    times = []
    
    for _ in range(iterations):
        start_time = time.perf_counter()
        resolved_path = project.get_path(filename)
        end_time = time.perf_counter()
        times.append(end_time - start_time)
    
    return times

# Run benchmark
single_call_times = benchmark_single_call(p, "test1.txt")

avg_time = mean(single_call_times)
std_time = stdev(single_call_times)
min_time = min(single_call_times)
max_time = max(single_call_times)

print(f"Single get_path() call performance ({len(single_call_times)} iterations):")
print(f"  Average: {avg_time*1000:.2f} ms")
print(f"  Std Dev: {std_time*1000:.2f} ms")
print(f"  Min:     {min_time*1000:.2f} ms")
print(f"  Max:     {max_time*1000:.2f} ms")

# Performance target validation
target_time = 0.1  # 100ms target
if avg_time < target_time:
    print(f"✓ Performance target met (< {target_time*1000:.0f}ms)")
else:
    print(f"✗ Performance target missed (> {target_time*1000:.0f}ms)")
```

## Multiple Calls Performance

Test performance with multiple consecutive calls:

```{python}
# Benchmark multiple calls
def benchmark_multiple_calls(project, filenames, iterations=10):
    times = []
    
    for _ in range(iterations):
        start_time = time.perf_counter()
        for filename in filenames:
            project.get_path(filename)
        end_time = time.perf_counter()
        times.append((end_time - start_time) / len(filenames))
    
    return times

# Run multiple calls benchmark
multi_call_times = benchmark_multiple_calls(p, test_files)

avg_multi = mean(multi_call_times)
total_calls = len(multi_call_times) * len(test_files)

print(f"Multiple get_path() calls performance:")
print(f"  Files per iteration: {len(test_files)}")
print(f"  Total calls tested: {total_calls}")
print(f"  Average per call: {avg_multi*1000:.2f} ms")

# Performance scaling analysis
if avg_multi < avg_time * 1.1:  # Allow 10% overhead
    print("✓ Multiple calls scale well (minimal overhead)")
else:
    print("⚠ Multiple calls show scaling overhead")
```

## Missing Files Performance

Test performance impact of missing files:

```{python}
# Benchmark missing files
missing_files = [f"missing_file_{i}.txt" for i in range(5)]

def benchmark_missing_files(project, filenames):
    times = []
    
    for filename in filenames:
        start_time = time.perf_counter()
        resolved_path = project.get_path(filename)
        end_time = time.perf_counter()
        times.append(end_time - start_time)
    
    return times

missing_file_times = benchmark_missing_files(p, missing_files)

avg_missing = mean(missing_file_times)
print(f"Missing files performance:")
print(f"  Average per missing file: {avg_missing*1000:.2f} ms")

# Compare to existing files
ratio = avg_missing / avg_time
print(f"  Missing file overhead: {ratio:.1f}x slower than existing files")

# Performance threshold
missing_threshold = 0.2  # 200ms threshold
if avg_missing < missing_threshold:
    print(f"✓ Missing file performance acceptable (< {missing_threshold*1000:.0f}ms)")
else:
    print(f"⚠ Missing file performance slow (> {missing_threshold*1000:.0f}ms)")
```

## Directory Search Impact

Test performance impact of directory search order:

```{python}
# Create files in different directories to test search priority impact
os.makedirs(os.path.join(tmp_dir, "intermediate"), exist_ok=True)
os.makedirs(os.path.join(tmp_dir, "input"), exist_ok=True)

# File in current directory (found first)
current_file = "priority_test_current.txt"
with open(os.path.join(tmp_dir, current_file), 'w') as f:
    f.write("current")

# File in intermediate directory (found second)
intermediate_file = "priority_test_intermediate.txt"
with open(os.path.join(tmp_dir, "intermediate", intermediate_file), 'w') as f:
    f.write("intermediate")

# File in input directory (found third)  
input_file = "priority_test_input.txt"
with open(os.path.join(tmp_dir, "input", input_file), 'w') as f:
    f.write("input")

# Benchmark directory search impact
def benchmark_directory_priority():
    files = [current_file, intermediate_file, input_file]
    times = {}
    
    for filename in files:
        start_time = time.perf_counter()
        resolved_path = p.get_path(filename)
        end_time = time.perf_counter()
        times[filename] = end_time - start_time
    
    return times

priority_times = benchmark_directory_priority()

print("Directory search priority performance:")
for filename, search_time in priority_times.items():
    location = "current" if "current" in filename else                "intermediate" if "intermediate" in filename else "input"
    print(f"  {location:12} directory: {search_time*1000:.2f} ms")
```

## Performance Monitoring Function

Example of performance monitoring in production:

```{python}
def monitor_get_path_performance(project, filename, warn_threshold=0.1):
    # Monitor get_path performance and warn if slow
    start_time = time.perf_counter()
    result = project.get_path(filename)
    end_time = time.perf_counter()
    
    duration = end_time - start_time
    
    if duration > warn_threshold:
        print(f"⚠ SLOW: get_path('{filename}') took {duration*1000:.2f}ms")
    else:
        print(f"✓ get_path('{filename}') completed in {duration*1000:.2f}ms")
    
    return result, duration

# Example usage
result, duration = monitor_get_path_performance(p, "test1.txt")
```

## Performance Summary

```{python}
print("\n" + "="*60)
print("PERFORMANCE SUMMARY")
print("="*60)

print(f"Single call average:     {avg_time*1000:8.2f} ms")
print(f"Multiple calls average:  {avg_multi*1000:8.2f} ms")
print(f"Missing files average:   {avg_missing*1000:8.2f} ms")

print(f"\nPerformance targets:")
print(f"  Single call < 100ms:    {'✓ PASS' if avg_time < 0.1 else '✗ FAIL'}")
print(f"  Multiple calls < 10ms:  {'✓ PASS' if avg_multi < 0.01 else '✗ FAIL'}")
print(f"  Missing files < 200ms:  {'✓ PASS' if avg_missing < 0.2 else '✗ FAIL'}")
```

## Optimization Tips

```{python}
optimization_tips = [
    "1. Place frequently accessed files in current or intermediate directories",
    "2. Use specific relative paths to minimize directory searching",
    "3. Cache get_path() results for repeated access to same files",
    "4. Consider file existence before calling get_path() for better error handling",
    "5. Use batch operations when processing multiple files"
]

print("Performance Optimization Recommendations:")
for tip in optimization_tips:
    print(f"  {tip}")
```

## Summary

This documentation demonstrates performance characteristics from the `TestPerformanceBenchmarks` test suite:

- **Single Call Performance**: < 100ms per call target
- **Multiple Calls**: Scales well with minimal overhead
- **Missing Files**: Slightly slower but still acceptable (< 200ms)
- **Directory Priority**: Current directory fastest, deeper searches slower
- **Monitoring**: Built-in performance tracking patterns
- **Optimization**: Clear recommendations for high-performance workflows

All benchmarks are based on working test cases that validate Hazelbean's performance characteristics.

## Cleanup

```{python}
# Clean up the temporary directory
import shutil
shutil.rmtree(tmp_dir)
print("Temporary test directory cleaned up")
```
