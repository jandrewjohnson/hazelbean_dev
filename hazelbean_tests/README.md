# Hazelbean Testing Infrastructure

> **Comprehensive testing suite for hazelbean geospatial processing library**
> 
> **Architecture:** Component-based testing with organized workflows for CI/CD integration

## Quick Start

### Essential Commands for Immediate Testing

```bash
# Prerequisites: Activate conda environment
conda activate hazelbean_env

# Smoke test - basic functionality validation (30 seconds)
python -m pytest hazelbean_tests/system/smoke/test_smoke.py -v

# Quick unit tests - core functionality validation (60 seconds)
python -m pytest hazelbean_tests/unit/test_get_path.py hazelbean_tests/unit/test_tile_iterator.py -v

# Integration validation - key workflows (90 seconds)
python -m pytest hazelbean_tests/integration/test_data_processing.py::TestGetPathIntegration -v

# Performance benchmarks - quality gate validation (60 seconds)
python -m pytest hazelbean_tests/performance/benchmarks/test_simple_benchmarks.py -v --benchmark-only
```

## Test Categories Overview

### ðŸŽ¯ **Unit Tests** (`unit/`) - Component-Based Testing
- **Organization**: By hazelbean module components (supports both flat and nested structure)
- **Total**: 74 tests across 20+ components
- **Focus Areas**: get_path (27 tests), tile_iterator (14 tests)
- **Success Rate**: 96% (72/74 passing, 2 documented bugs)
- **Architecture**: Component isolation with comprehensive coverage
- **âœ¨ Hierarchical Support**: Mixed flat (`test_get_path.py`) and nested (`get_path/test_*.py`) organization

```bash
# Complete unit test suite
python -m pytest hazelbean_tests/unit/ -v

# Priority components (100% pass rate)
python -m pytest hazelbean_tests/unit/test_get_path.py -v          # Path resolution
python -m pytest hazelbean_tests/unit/test_tile_iterator.py -v     # Spatial tiling
```

### ðŸ”„ **Integration Tests** (`integration/`) - Workflow-Based Testing
- **Organization**: By cross-component workflows
- **Total**: 71 tests across 5 workflow categories
- **Success Rate**: 37% (26/71 passing, infrastructure issues identified)
- **Focus**: Data processing, end-to-end workflows, project flows

```bash
# Complete integration test suite
python -m pytest hazelbean_tests/integration/ -v

# Reliable workflow categories
python -m pytest hazelbean_tests/integration/test_data_processing.py -v
python -m pytest hazelbean_tests/integration/test_end_to_end_workflow.py -v
```

### âš¡ **Performance Tests** (`performance/`) - Quality Gate Testing
- **Organization**: By functions, workflows, and benchmarks
- **Total**: 68 tests across 4 categories
- **Infrastructure**: Baseline management, regression detection
- **Output**: Interactive dashboards, CSV exports

```bash
# Complete performance workflow (recommended)
python scripts/run_performance_benchmarks.py --complete-workflow --runs 2

# Quick benchmark execution
python -m pytest hazelbean_tests/performance/benchmarks/test_simple_benchmarks.py -v --benchmark-only
```

### ðŸ–¥ï¸ **System Tests** (`system/`) - Infrastructure Testing
- **Organization**: Environment, CLI, installation validation
- **Focus**: Smoke tests, CLI interface, environment validation
- **Purpose**: CI/CD pipeline validation

```bash
# System smoke tests
python -m pytest hazelbean_tests/system/smoke/ -v

# CLI interface validation
python -m pytest hazelbean_tests/system/cli/ -v
```

### ðŸ› ï¸ **Tools** (`tools/`) - Testing Infrastructure
- **QMD Automation**: Documentation generation from tests with intelligent path resolution
  - âœ¨ **Mixed organization**: Handles both flat and nested test files
  - âœ¨ **Collision prevention**: Automatically generates unique documentation names
  - âœ¨ **Category filtering**: Process specific test categories (`--category unit|integration`)
  - âœ¨ **Enhanced navigation**: Hierarchical breadcrumbs in generated documentation
  - ðŸ”§ **Path resolution**: Auto-fixes `sys.path` hacks and standardizes data paths
  - ðŸ”§ **Data dependencies**: Validates file access and generates comprehensive dependency docs
- **Fixtures & Data**: Shared testing resources
- **Utilities**: Helper functions and testing tools

## Environment Setup

### Prerequisites
```bash
# 1. Activate conda environment (REQUIRED)
conda activate hazelbean_env

# 2. Verify environment
python -c "import hazelbean as hb; print('âœ… Hazelbean available')"
python -c "import pytest; print('âœ… PyTest available')"

# 3. Install additional testing dependencies (if needed)
conda install -c conda-forge pytest-benchmark pytest-cov
```

### Critical Environment Notes
- **Always use `python -m pytest`** (not direct `pytest`) to avoid taskgraph import issues [[memory:7779366]]
- **Use conda/mamba for package installation** (not pip) to ensure correct environment [[memory:7779366]]
- **Scripts assume conda environment is active** - no need for explicit activation commands in automation [[memory:7966990]]

## Known Bug Test Handling

### Using xfail for Known Bugs

Some tests are marked with `@pytest.mark.xfail` because they expose known bugs in hazelbean core code (see `KNOWN_BUGS.md`). This approach:

- âœ… **Allows CI to pass** - xfailed tests don't fail the build
- âœ… **Tests still run** - We catch unexpected behavior changes
- âœ… **Automatic bug-fix detection** - Tests show "XPASSED" when bugs are fixed
- âœ… **Clear tracking** - Percentage of known vs. unexpected failures

### What xfail Means

```python
@pytest.mark.xfail(
    reason="Known bug: project_flow.py:772 - See KNOWN_BUGS.md",
    strict=True,  # Test will report XPASS if bug gets fixed
    raises=AttributeError  # Current buggy behavior
)
def test_something(self):
    # Test expects CORRECT behavior (TypeError)
    # But hazelbean currently has bug (raises AttributeError)
    # So test is expected to fail until bug is fixed
```

### When Bugs Are Fixed

When you fix a bug in hazelbean:

1. **Tests will show XPASSED** - "Unexpectedly passed"
2. **Remove the xfail decorator** from the test
3. **Update KNOWN_BUGS.md** - Change status to âœ… Fixed
4. **Test runs normally** as a passing test

### Current Known Bugs

See `KNOWN_BUGS.md` for complete list. Current xfailed tests:

- **add_iterator error handling** (4 tests)
  - `test_add_iterator_invalid_function_error`
  - `test_add_iterator_none_function_error`
  - `test_non_callable_function_parameter`
  - `test_none_function_parameter`

### Viewing Failure Rates

```bash
# Install pytest-json-report if needed
conda install -c conda-forge pytest-json-report

# Run tests with JSON report
pytest hazelbean_tests/unit/test_add_iterator.py \
  --json-report \
  --json-report-file=test-results.json

# Generate failure rate report
python tools/test_failure_tracking.py test-results.json

# Compare to baseline
python tools/test_failure_tracking.py test-results.json \
  --compare-to metrics/test-failures/baseline-add_iterator.json
```

### CI Tracking

GitHub Actions automatically tracks failure rates:
- View in "Actions" tab â†’ Workflow run â†’ "Test Results Summary"
- Historical trends in `metrics/test-failures/`
- Alerts on unexpected failures or bug fixes

---

## Common Testing Workflows

### 1. **Developer Quick Validation** (5 minutes)
```bash
conda activate hazelbean_env

# Smoke test for immediate feedback
python -m pytest hazelbean_tests/system/smoke/test_smoke.py -v --tb=short

# Core functionality validation
python -m pytest hazelbean_tests/unit/test_get_path.py -k "test_local" -v --tb=short
python -m pytest hazelbean_tests/unit/test_get_path.py::TestCloudStorageIntegration -v --tb=short
```

### 2. **Pre-Commit Testing** (15 minutes)
```bash
conda activate hazelbean_env

# Unit tests for core components
python -m pytest hazelbean_tests/unit/ -v --tb=short

# Integration tests for changed workflows
python -m pytest hazelbean_tests/integration/test_data_processing.py -v --tb=short

# Performance regression check
python -m pytest hazelbean_tests/performance/benchmarks/test_simple_benchmarks.py -v --benchmark-only
```

### 3. **Full Quality Gate Validation** (30+ minutes)
```bash
conda activate hazelbean_env

# Complete test suite execution
python -m pytest hazelbean_tests/ -v --tb=short --cov=hazelbean

# Performance baseline establishment
python scripts/run_performance_benchmarks.py --complete-workflow --runs 3

# Generate comprehensive reporting
python scripts/create_benchmark_dashboard.py
```

## CI/CD Integration Guidelines

### GitHub Actions Integration

#### Basic Test Pipeline
```yaml
name: Test Suite
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Mambaforge with hazelbean_env
        uses: mamba-org/setup-micromamba@v1
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          cache-environment: true
          
      - name: Run smoke tests (Quality Gate 1)
        run: python -m pytest hazelbean_tests/system/smoke/ -v --tb=short
        
      - name: Run unit tests (Quality Gate 2)
        run: python -m pytest hazelbean_tests/unit/ -v --tb=short --cov=hazelbean
        
      - name: Run integration tests (Quality Gate 3)
        run: python -m pytest hazelbean_tests/integration/ -v --tb=short --maxfail=10
        
      - name: Performance regression check (Quality Gate 4)
        run: python -m pytest hazelbean_tests/performance/benchmarks/ -v --benchmark-only
```

#### Advanced Pipeline with Quality Gates
```yaml
name: Advanced Quality Gates
on: [push, pull_request]

jobs:
  quality-gates:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, "3.10", "3.11", "3.12"]
        
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Environment
        uses: mamba-org/setup-micromamba@v1
        with:
          environment-file: environment.yml
          environment-name: hazelbean_env
          
      - name: Quality Gate 1 - Infrastructure
        run: |
          python -m pytest hazelbean_tests/system/smoke/ -v
          echo "âœ… Infrastructure validation passed"
          
      - name: Quality Gate 2 - Core Functionality
        run: |
          python -m pytest hazelbean_tests/unit/test_get_path.py hazelbean_tests/unit/test_tile_iterator.py -v
          if [ $? -eq 0 ]; then echo "âœ… Core functionality validation passed"; fi
          
      - name: Quality Gate 3 - Integration Workflows
        run: |
          python -m pytest hazelbean_tests/integration/test_data_processing.py::TestGetPathIntegration -v
          if [ $? -eq 0 ]; then echo "âœ… Integration workflow validation passed"; fi
          
      - name: Quality Gate 4 - Performance Regression
        run: |
          python scripts/run_performance_benchmarks.py --establish-baseline --runs 2
          if [ $? -eq 0 ]; then echo "âœ… Performance regression check passed"; fi
          
      - name: Generate Quality Report
        if: always()
        run: |
          python scripts/create_benchmark_dashboard.py --ci-mode
          echo "ðŸ“Š Quality report generated"
```

## Quality Gate Implementation

### 1. **Test Coverage Thresholds**
```bash
# Minimum test pass rate requirements
--cov-fail-under=0  # Currently permissive (infrastructure development phase)
# Recommended production: --cov-fail-under=80
```

### 2. **Performance Regression Detection**
```bash
# Acceptable performance degradation limits
PERFORMANCE_REGRESSION_THRESHOLD=20%  # 20% degradation triggers failure
BASELINE_CONFIDENCE_INTERVAL=95%      # Statistical significance requirement
```

### 3. **Integration Validation Criteria**
```bash
# End-to-end workflow success criteria
SMOKE_TESTS_PASS_RATE=100%           # All smoke tests must pass
CORE_FUNCTIONALITY_PASS_RATE=95%     # get_path, tile_iterator critical
INTEGRATION_WORKFLOW_PASS_RATE=50%   # Infrastructure issues acceptable
```

### 4. **Quality Gate Failure Handling**

#### Critical Failures (Block Deployment)
- Smoke tests failing
- Core functionality regression (get_path, tile_iterator)
- Performance degradation >20%

#### Warning Failures (Review Required)
- Integration test infrastructure issues
- Coverage drop >5%
- New test failures in non-critical components

#### Informational (Monitoring Only)
- Documentation generation issues
- Non-critical performance variations <10%
- Edge case test failures with known root causes

## Testing Philosophy & Best Practices

### Test Failure Approach [[memory:8070712]]
- **Infrastructure issues** should be fixed in test code, not hazelbean
- **Edge cases/bugs** in hazelbean are acceptable and should be documented
- **Test setup errors** are NOT acceptable and must be resolved

### Quality Gate Success Stories
The testing infrastructure has successfully identified real bugs:
- 2 legitimate bugs discovered in hazelbean POG validation logic
- 100% setup issue resolution achieved
- Professional systematic methodology implemented

### Critical Thinking Approach [[memory:7968771]]
- Tests should challenge assumptions and provide honest feedback
- Quality gates should push back on problematic changes
- Systematic approach preferred over blind agreement

## Troubleshooting Guide

### Common Issues & Solutions

#### Environment Issues
```bash
# Issue: ModuleNotFoundError for taskgraph, hazelbean, etc.
# Solution: Ensure conda environment is activated
conda activate hazelbean_env
which python  # Should show conda environment path

# Issue: pytest command not found or wrong version
# Solution: Use python -m pytest instead of direct pytest
python -m pytest --version
```

#### GDAL Environment Issues
```bash
# Issue: GDAL data directory errors in integration tests
# Temporary: Skip vector tests
python -m pytest hazelbean_tests/integration/ -k "not (super_simplify or zonal_statistics)"

# Permanent fix: Reinstall GDAL
conda install -c conda-forge --force-reinstall gdal
```

#### Test Data Issues
```bash
# Issue: Missing test data files
# Solution: Verify data directory exists and contains required files
ls -la data/tests/
# Should contain: valid_cog_example.tif, invalid_cog_example.tif

# Temporary: Skip data-dependent tests
python -m pytest hazelbean_tests/ -k "not reading_csvs"
```

#### Performance Benchmark Issues
```bash
# Issue: "benchmarks_dir not defined" 
# Status: âœ… FIXED in Story 7 implementation

# Issue: JSON corruption in benchmark files
# Solution: System gracefully handles this - not blocking for CI/CD

# Issue: Performance variance too high
# Solution: Increase baseline runs for better statistical confidence
python scripts/run_performance_benchmarks.py --establish-baseline --runs 5
```

### CI/CD Specific Issues

#### GitHub Actions Environment Setup
```bash
# Issue: Conda environment not activating correctly
# Solution: Use shell configuration in workflow
defaults:
  run:
    shell: bash -l {0}

# Issue: Environment caching not working
# Solution: Clear cache and regenerate
# GitHub > Settings > Actions > Caches > Delete All
```

#### Permission Issues
```bash
# Issue: Unable to commit documentation updates
# Solution: Ensure GitHub token has proper permissions
permissions:
  contents: write      # For commits
  pull-requests: write # For PR comments
  actions: read        # For reading workflow state
```

## Project Structure

```
hazelbean_tests/
â”œâ”€â”€ README.md                    # This comprehensive guide
â”œâ”€â”€ conftest.py                  # Global test configuration
â”œâ”€â”€ pytest.ini                  # Pytest configuration (in project root)
â”‚
â”œâ”€â”€ (setup-guides/)             # ðŸ“‹ Setup & Command Guides (moved to docs/getting-started.md)
â”‚   â”œâ”€â”€ README.md               # Guide overview and quick reference
â”‚   â”œâ”€â”€ foundation-setup.md     # Complete architecture setup
â”‚   â”œâ”€â”€ unit-test-demo-commands.md      # Unit testing workflows
â”‚   â”œâ”€â”€ integration-test-demo-commands.md # Integration testing workflows
â”‚   â”œâ”€â”€ performance-test-demo-commands.md # Performance testing workflows
â”‚   â””â”€â”€ file-movement-documentation.md   # Migration history
â”‚
â”œâ”€â”€ unit/                        # Component-based unit tests
â”‚   â”œâ”€â”€ get_path/               # Path resolution (27 tests, 100% pass)
â”‚   â”œâ”€â”€ tile_iterator/          # Spatial tiling (14 tests, 100% pass)
â”‚   â”œâ”€â”€ arrayframe/             # Array operations
â”‚   â”œâ”€â”€ [...]                  # 17 more component directories
â”‚   â””â”€â”€ README.md               # Unit test documentation
â”‚
â”œâ”€â”€ integration/                # Workflow-based integration tests
â”‚   â”œâ”€â”€ data_processing_workflows/    # Core data operations
â”‚   â”œâ”€â”€ end_to_end_workflows/        # Complete workflow testing
â”‚   â”œâ”€â”€ project_flow_workflows/      # Project flow integration
â”‚   â”œâ”€â”€ parallel_processing_workflows/ # Parallel processing
â”‚   â””â”€â”€ README.md               # Integration test documentation
â”‚
â”œâ”€â”€ performance/                # Performance testing & benchmarking
â”‚   â”œâ”€â”€ benchmarks/             # Integration scenarios (21 tests)
â”‚   â”œâ”€â”€ functions/              # Function-level benchmarks (33 tests)
â”‚   â”œâ”€â”€ workflows/              # Workflow performance (8 tests)
â”‚   â”œâ”€â”€ baseline/               # Baseline management system
â”‚   â””â”€â”€ README.md               # Performance test documentation
â”‚
â”œâ”€â”€ system/                     # System-level testing
â”‚   â”œâ”€â”€ smoke/                  # Basic functionality validation
â”‚   â”œâ”€â”€ cli/                    # Command-line interface tests
â”‚   â”œâ”€â”€ environment/            # Environment validation
â”‚   â””â”€â”€ installation/           # Installation testing
â”‚
â””â”€â”€ tools/                      # Testing infrastructure
    â”œâ”€â”€ qmd_automation/         # ðŸ“š Documentation generation system
    â”‚   â”œâ”€â”€ README.md           # QMD system overview & best practices
    â”‚   â”œâ”€â”€ cli.py              # Main CLI interface
    â”‚   â”œâ”€â”€ config/             # Configuration management
    â”‚   â”œâ”€â”€ core/               # Core processing engines
    â”‚   â”œâ”€â”€ docs/               # Detailed system documentation
    â”‚   â””â”€â”€ templates/          # Quarto templates
    â”œâ”€â”€ fixtures/               # Shared test fixtures
    â”œâ”€â”€ data/                   # Test data management
    â””â”€â”€ utilities/              # Testing helper functions
```

## Getting Help

### Documentation Resources
- **Getting Started**: [../docs/getting-started.md](../docs/getting-started.md) - Consolidated setup guide
- **Educational System**: [../docs/educational-system.md](../docs/educational-system.md) - Tutorial generation system  
- **Project Status**: [../docs/project-status.md](../docs/project-status.md) - Current development status
- **Architecture**: [../docs/technical_architecture.md](../docs/technical_architecture.md) - System design
- **Setup Guides** (consolidated): Content moved to [Getting Started Guide](../docs/getting-started.md)
- **QMD Automation**: [tools/qmd_automation/README.md](tools/qmd_automation/README.md)

### Status & Monitoring
- **Current Status**: [../docs/project-status.md](../docs/project-status.md) - Consolidated project status
- **Historical Status** (legacy): `status/` directory - Historical status reports
- **Performance Metrics**: `../metrics/` directory

### Contact & Support
- **Testing Infrastructure Issues**: Review troubleshooting guide above
- **Hazelbean Functionality Questions**: Refer to main project documentation
- **CI/CD Integration Support**: Use GitHub Actions templates provided

---

> **Note**: This testing infrastructure represents comprehensive quality gates for hazelbean development. It follows professional systematic methodologies and provides reliable feedback for both local development and CI/CD automation.
